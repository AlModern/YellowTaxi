{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"text-align:right\">Алексей Бебчик&nbsp;&nbsp;</div><b>\n",
    "### Курс 6. Желтое такси. Неделя 6. Дополнительные признаки  \n",
    "(задание, оцениваемое сокурсниками)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.coursera.org/learn/data-analysis-project/peer/gtNEG/dopolnitiel-nyie-priznaki\">Задание</a>\n",
    "\n",
    "На этой неделе вам предстоит попробовать добавить в вашу регрессионную модель дополнительные признаки.\n",
    "\n",
    "Во-первых, для прогнозирования можно использовать информацию, содержащуюся в сырых данных:\n",
    "- средняя длительность поездок;  \n",
    "- среднее количество пассажиров;  \n",
    "- среднее расстояние по счётчику;  \n",
    "- доли географических зон, в которые совершаются поездки;  \n",
    "- доли поездок, совершаемых по тарифам каждого из типов;  \n",
    "- доли способов оплаты поездок;  \n",
    "- средняя стоимость поездок;  \n",
    "- доли провайдеров данных.\n",
    "\n",
    "Все эти признаки можно использовать только с задержкой, то есть, при прогнозировании $\\hat{y}_{T+i|T}$ эти признаки должны быть рассчитаны по данным не позднее момента времени $T$. Каждый из этих признаков можно использовать по-разному: как сырые значения за последние несколько часов, так и средние за последний день, неделю, месяц и т.д.  \n",
    "\n",
    "Во-вторых, чтобы улучшить качество прогнозов в аномальные периоды, вы можете найти информацию о потенциально влияющих на количество поездок событиях, таких, как государственные праздники. Проанализируйте, как именно поведение пассажиров меняется во время этих событий, и создайте признаки, отражающие эти изменения. Как показывает наш опыт, правильный учёт праздничных дней часто позволяет существенно уменьшить среднюю ошибку прогноза.  \n",
    "\n",
    "В-третьих, можно использовать признаки, связанные с географией. Например, скорее всего, суммарное количество поездок, совершаемых из географической зоны, пропорционально площади этой зоны. Для зон, прилегающих к аэропорту, может быть характерен специфический паттерн дневной сезонности, связанный с тем, что спрос на такси будет повышаться в те часы, когда общественный транспорт перестаёт работать. В деловом центре максимальное количество поездок будет приходиться на начало и окончание рабочего дня, на Бродвее — на время начала и окончания спектаклей. Все эти идеи не обязательно верны, мы приводим их здесь только для того, чтобы продемонстрировать принцип рассуждений. Ещё один пример географического признака: можно попробовать добавить идентификатор боро, который можно найти в файле https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv. Кроме того, нам кажется перспективным использование в качестве фактора количества поездок, совершённых за прошлый час/день и т. д. из соседних географических зон, или количества поездок, совершённых за прошлый час/день в текущую географическую зону.    \n",
    "\n",
    "Много примеров других признаков, которые можно использовать при регрессионном прогнозировании, можно найти в лекции Вадима Стрижова.  \n",
    "\n",
    "Чтобы сдать задание, выполните следующую последовательность действий.    \n",
    "\n",
    "Загрузите обучающие выборки прошлой недели, перечислите используемые в моделях признаки и посчитайте $Q_{may}$ — качество прогнозов моделей, настроенных на данных до апреля 2016, в мае 2016.  \n",
    "\n",
    "Попробуйте добавить признаки. Используйте идеи, которые мы предложили, или какие-то свои. Обучайте обновлённые модели на данных до апреля 2016 включительно и считайте качество новых прогнозов на мае. Удаётся ли вам улучшить качество? Не нужно ли увеличить сложность регрессионной модели? Если добавляемый признак не улучшает качество, всё равно оставьте доказательства этому в ноутбуке, чтобы ваши коллеги это видели при проверке.   \n",
    "\n",
    "Когда вы примете решение остановиться и перестать добавлять признаки, постройте для каждой географической зоны и каждого конца истории от 2016.04.30 23:00 до 2016.05.31 17:00 прогнозы на 6 часов вперёд; посчитайте в ноутбуке ошибку прогноза по следующему функционалу:  \n",
    "$Q_{may} =\\frac1{R* 739* 6} \\sum\\limits_{r=1}^{R} \\sum_{T=2016.04.30 23:00}^{2016.05.31 17:00} \\sum_{i=1}^6 \\left| \\hat{y}^r_{T|T+i} - y^r_{T+i} \\right|$.  \n",
    "\n",
    "Убедитесь, что среднее качество прогнозов увеличилось.\n",
    "Переобучите итоговые модели на данных до мая 2016 включительно, постройте прогнозы на июнь для каждого конца истории от 2016.05.31 23:00 до 2016.06.30 17:00 и запишите все результаты в один файл в уже знакомом вам формате: $geoID, histEndDay, histEndHour, step, y$  \n",
    "\n",
    "Загрузите полученный файл на kaggle: https://inclass.kaggle.com/c/yellowtaxi. Добавьте в ноутбук ссылку на сабмишн.\n",
    "Загрузите ноутбук в форму.  \n",
    "\n",
    "__Review criteria__  \n",
    "В качестве ответа в этом задании вам нужно загрузить ноутбук; убедитесь, что ход анализа, который вы провели, описан достаточно подробно для того, чтобы ваши сокурсники поняли, что вы делали и почему. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import os                                     #для проверки существования файлов\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model, ensemble, metrics\n",
    "import datetime as dt\n",
    "# import seaborn as sns \n",
    "import timeit as ti\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from time import sleep\n",
    "\n",
    "from datetime import date\n",
    "import holidays\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path  = '..\\\\YTData\\\\'  #путь к папке с данными \"K6w6_290718\"\n",
    "data6_path = 'YTw6_data\\\\'   #путь к папке с данными "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_time(label='now'):\n",
    "    print label, dt.datetime.now().strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Загружаем подготовленные данные из файла\n",
    "data_ext = pd.read_csv(data6_path + 'YTw6_data_ext.csv', parse_dates=['dt_hour'], dayfirst=True) \n",
    "list_region = list(data_ext.region.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(data6_path + 'YTw6_data0.csv', parse_dates=['dt_hour'], dayfirst=True) #Загружаем подготовленные данные из файла\n",
    "list_region = list(data0.region.unique())\n",
    "# Составьте из данных о поездках прямоугольную таблицу так, чтобы по строкам было время, а по столбцам идентификатор ячейки\n",
    "data = pd.pivot_table(data0, values='trips', index =['dt_hour'], columns=['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>region</th>\n",
       "      <th>1075</th>\n",
       "      <th>1076</th>\n",
       "      <th>1077</th>\n",
       "      <th>1125</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "      <th>1129</th>\n",
       "      <th>1130</th>\n",
       "      <th>1131</th>\n",
       "      <th>...</th>\n",
       "      <th>1630</th>\n",
       "      <th>1684</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1783</th>\n",
       "      <th>2068</th>\n",
       "      <th>2069</th>\n",
       "      <th>2118</th>\n",
       "      <th>2119</th>\n",
       "      <th>2168</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_hour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>80</td>\n",
       "      <td>144</td>\n",
       "      <td>50</td>\n",
       "      <td>77</td>\n",
       "      <td>319</td>\n",
       "      <td>402</td>\n",
       "      <td>531</td>\n",
       "      <td>617</td>\n",
       "      <td>846</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>91</td>\n",
       "      <td>211</td>\n",
       "      <td>49</td>\n",
       "      <td>134</td>\n",
       "      <td>404</td>\n",
       "      <td>420</td>\n",
       "      <td>370</td>\n",
       "      <td>453</td>\n",
       "      <td>594</td>\n",
       "      <td>224</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>90</td>\n",
       "      <td>146</td>\n",
       "      <td>23</td>\n",
       "      <td>110</td>\n",
       "      <td>393</td>\n",
       "      <td>425</td>\n",
       "      <td>313</td>\n",
       "      <td>366</td>\n",
       "      <td>377</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>32</td>\n",
       "      <td>87</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>399</td>\n",
       "      <td>324</td>\n",
       "      <td>309</td>\n",
       "      <td>327</td>\n",
       "      <td>166</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>145</td>\n",
       "      <td>254</td>\n",
       "      <td>264</td>\n",
       "      <td>333</td>\n",
       "      <td>318</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "region               1075  1076  1077  1125  1126  1127  1128  1129  1130  \\\n",
       "dt_hour                                                                     \n",
       "2016-01-01 00:00:00    80   144    50    77   319   402   531   617   846   \n",
       "2016-01-01 01:00:00    91   211    49   134   404   420   370   453   594   \n",
       "2016-01-01 02:00:00    90   146    23   110   393   425   313   366   377   \n",
       "2016-01-01 03:00:00    32    87    16    62   252   399   324   309   327   \n",
       "2016-01-01 04:00:00    24    43    10    53   145   254   264   333   318   \n",
       "\n",
       "region               1131  ...   1630  1684  1733  1734  1783  2068  2069  \\\n",
       "dt_hour                    ...                                              \n",
       "2016-01-01 00:00:00   267  ...     12     0     2    44     5    41     4   \n",
       "2016-01-01 01:00:00   224  ...     29     0     5     2     2     4     0   \n",
       "2016-01-01 02:00:00   138  ...     47     0     3     0     4     0     0   \n",
       "2016-01-01 03:00:00   166  ...     46     0     2     4     5     1     0   \n",
       "2016-01-01 04:00:00   145  ...     43     0     0     1     1     0     0   \n",
       "\n",
       "region               2118  2119  2168  \n",
       "dt_hour                                \n",
       "2016-01-01 00:00:00    70     7    66  \n",
       "2016-01-01 01:00:00    47     1    29  \n",
       "2016-01-01 02:00:00    69     1    14  \n",
       "2016-01-01 03:00:00    21     0     9  \n",
       "2016-01-01 04:00:00    26     1     6  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_values = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЧАСТЬ 1. Признаки__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой из шести задач прогнозирования $ŷ T+i∣T,i=1,…,6$ $y^T+i∣T,i=1,…,6$ сформируйте выборки. Откликом будет  $yT+i$ при всевозможных значениях $T$ , а признаки можно использовать следующие:  \n",
    "• идентификатор географической зоны — категориальный;  \n",
    "• год, месяц, день месяца, день недели, час — эти признаки можно пробовать брать и категориальными, и непрерывными, можно даже и так, и так;  \n",
    "• синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA;  \n",
    "• сами значения прогнозов ARIMA $\\hat{y}_{T+i|T}^{ARIMA}$;  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T},y_{T-1}, \\dots, y_{T-K}$ (параметр $K$ можно подбирать; попробуйте начать, например, с 6);  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T-24}, y_{T-48}, \\dots, y_{T-24*K_d}$ (параметр $K_d$ можно подбирать; попробуйте начать, например, с 2);  \n",
    "• суммарное количество поездок из рассматриваемого района за предшествующие полдня, сутки, неделю, месяц.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_endtime = '2016.04.30 17:00'\n",
    "tune_begtime  = '2016.04.30 23:00'\n",
    "tune_endtime  = '2016.05.31 17:00'\n",
    "test_begtime  = '2016.05.31 23:00'\n",
    "test_endtime  = '2016.06.30 17:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 250)\n",
    "pd.set_option('display.max_rows', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_regs = pd.read_csv('k6w6_reg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема формирования признаков сформирована по мотивам работы Ушакова Вадима Константиновича. Предложенный коллегой подход прост и эффективнен, так что представляется полезным использовать его в жизни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04e58312c941258db726f564bba907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "K = 23 # количество поездок за К часов\n",
    "K_d = 7 # количество поездок за К*24 часа\n",
    "k_week = 4 # количество Фурье компонент для недели\n",
    "k_pol = 3 # степень полинома\n",
    "Y1 = []\n",
    "Y2 = []\n",
    "Y3 = []\n",
    "Y4 = []\n",
    "Y5 = []\n",
    "Y6 = []\n",
    "times = []\n",
    "regions = []\n",
    "years = []\n",
    "months = []\n",
    "weeks = []\n",
    "hours = []\n",
    "hour_trips = []\n",
    "day_trips = []\n",
    "total_half_day = []\n",
    "total_day = []\n",
    "total_week = []\n",
    "total_month = []\n",
    "sin_week = []\n",
    "cos_week = []\n",
    "polinom = []\n",
    "weather = []\n",
    "\n",
    "#для каждого района\n",
    "for region in tqdm_notebook(range(102)):\n",
    "    #для каждой строки данных го ЧЕГО: От (ЧЕГО до КонцаМинус6Часов)\n",
    "    for j in range(1, len(data.index)-6): #range(len(data.index) - 8760*2, len(data.index)-6)\n",
    "        T = data.index[j] #ДатаВремя\n",
    "        times.append(T)  #Индекс-ДатаВремя\n",
    "        series = data_values[:j, region]\n",
    "        t = len(series)\n",
    "        # Фурье компоненты недели\n",
    "        sin_week.append([np.sin(t * 2 * pi * (i+1) / 168) for i in range(k_week)])\n",
    "        cos_week.append([np.cos(t * 2 * pi * (i+1) / 168) for i in range(k_week)])\n",
    "        polinom.append([t**(i+1) for i in range(k_pol)])\n",
    "        regions.append(list_region[region]) # идентификатор географической зоны\n",
    "        years.append(T.year)      # год\n",
    "        months.append(T.month)    # месяц\n",
    "        weeks.append(T.weekday()) # день недели\n",
    "        hours.append(T.hour)      # час\n",
    "\n",
    "        hour_trips.append([series[-1-t1] if len(series)-1-t1>=0 else -1 for t1 in range(K)]) # количество поездок за К часов\n",
    "\n",
    "        day_trips.append([series[-1-(t+1)*24] if len(series)-1-(t+1)*24>=0 else -1 for t in range(K_d)]) # количество поездок за К*24 часа\n",
    "        \n",
    "        total_half_day.append(series[-12:].sum()) # суммарное количество поездок за 12 часов\n",
    "        total_day.append(series[-24:].sum()) # суммарное количество поездок за день\n",
    "        total_week.append(series[-168:].sum()) # суммарное количество поездок за неделю\n",
    "        total_month.append(series[-730:].sum()) # суммарное количество поездок за месяц\n",
    "        \n",
    "        # Для ускорения создаем сразу все 6 таргетов \n",
    "        #!!!признаки используются как samples[:,-6])!!!\n",
    "        Y1.append(data_values[j+1, region])\n",
    "        Y2.append(data_values[j+2, region])\n",
    "        Y3.append(data_values[j+3, region])\n",
    "        Y4.append(data_values[j+4, region])\n",
    "        Y5.append(data_values[j+5, region])\n",
    "        Y6.append(data_values[j+6, region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['region', 'year','month','weekday','hour']\n",
    "columns.extend(['hour_trips'+str(i+1) for i in range(K)])\n",
    "columns.extend(['day_trips'+str(i+1) for i in range(K_d)])\n",
    "columns.extend(['sin_week'+str(i+1) for i in range(k_week)])\n",
    "columns.extend(['cos_week'+str(i+1) for i in range(k_week)])\n",
    "columns.extend(['polinom'+str(i+1) for i in range(k_pol)])    \n",
    "columns.extend(['total_half_day', 'total_day','total_week','total_month','y1','y2','y3','y4','y5','y6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_main_num = ['region', 'year','month','weekday','hour']\n",
    "cols_hour_val = ['hour_trips'+str(i+1) for i in range(K)]\n",
    "cols_day_val  = ['day_trips'+str(i+1) for i in range(K_d)]\n",
    "cols_sincos   = ['sin_week'+str(i+1) for i in range(k_week)] + ['cos_week'+str(i+1) for i in range(k_week)]\n",
    "cols_polinom  = ['polinom'+str(i+1) for i in range(k_pol)] \n",
    "cols_regsum   = ['total_half_day', 'total_day','total_week','total_month']\n",
    "cols_y        = ['y1','y2','y3','y4','y5','y6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cols = cols_main_num + cols_hour_val + cols_day_val + cols_sincos + cols_polinom + cols_regsum + cols_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_vals = np.column_stack((regions,years,months,weeks,hours,hour_trips,day_trips,\n",
    "                           sin_week,cos_week, polinom, total_half_day,total_day,total_week,total_month,Y1,Y2,Y3,Y4,Y5,Y6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data5_num = pd.DataFrame(all_vals, columns=all_cols,index=times) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Категориальные признаки \n",
    "data5_cat = pd.get_dummies(data5_num[cols_main_num], columns=cols_main_num)\n",
    "cols_main_cat = list(data5_cat.columns) #считываем получившиеся столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data5 = pd.read_csv('k6w6_data5.csv', parse_dates=['dt_hour'], dayfirst=True) #Загружаем подготовленные данные из файла\n",
    "# data5.index = data5load.dt_hour\n",
    "# data5.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5 = pd.concat([data5_num, data5_cat], axis=1)\n",
    "data5['region']  = data5.region.astype(int)\n",
    "data5['dt_hour'] = data5.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5.to_csv(data6_path + 'YTw6_data5.csv', index=False)  #сохраняем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6 = pd.merge(data5, data_ext, on = ['dt_hour','region']).fillna(0)\n",
    "data6.index = data6.dt_hour\n",
    "data6.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6.to_csv('k6w6_data6.csv', index=False)  #сохраняем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(445536, 21) (444822, 197) (444822, 216)\n"
     ]
    }
   ],
   "source": [
    "print data_ext.shape, data5.shape, data6.shape #(445536, 21) (444822, 197) (444822, 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data5 = pd.read_csv(data6_path + 'YTw6_data5.csv', parse_dates=['dt_hour'], dayfirst=True) #Загружаем подготовленные данные\n",
    "data5.index = data5.dt_hour\n",
    "data5.index.name = None\n",
    "\n",
    "data6 = pd.read_csv(data6_path + 'YTw6_data6.csv', parse_dates=['dt_hour'], dayfirst=True) #Загружаем подготовленные данные\n",
    "data6.index = data6.dt_hour\n",
    "data6.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Добавим новые признаки, извлеченные из данных (файл k6w6_append_data.ipynb)__  \n",
    "средняя длительность поездок;   \n",
    "среднее количество пассажиров;   \n",
    "среднее расстояние по счётчику;   \n",
    "доли поездок, совершаемых по тарифам каждого из типов (категориальный);  \n",
    "доли способов оплаты поездок (категориальный);                            \n",
    "средняя стоимость поездок;   \n",
    "доли провайдеров данных (категориальный).                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_y        = ['y1','y2','y3','y4','y5','y6']\n",
    "cols_polinom  = ['polinom1', 'polinom2', 'polinom3']\n",
    "cols_main_num = ['region', 'year', 'month', 'weekday', 'hour']\n",
    "cols_main_cat = ['region_1075.0', 'region_1076.0', 'region_1077.0', 'region_1125.0', 'region_1126.0', 'region_1127.0', 'region_1128.0', 'region_1129.0', 'region_1130.0', 'region_1131.0', 'region_1132.0', 'region_1172.0', 'region_1173.0', 'region_1174.0', 'region_1175.0', 'region_1176.0', 'region_1177.0', 'region_1178.0', 'region_1179.0', 'region_1180.0', 'region_1181.0', 'region_1182.0', 'region_1183.0', 'region_1184.0', 'region_1221.0', 'region_1222.0', 'region_1223.0', 'region_1224.0', 'region_1225.0', 'region_1227.0', 'region_1228.0', 'region_1229.0', 'region_1230.0', 'region_1231.0', 'region_1232.0', 'region_1233.0', 'region_1234.0', 'region_1235.0', 'region_1272.0', 'region_1273.0', 'region_1274.0', 'region_1278.0', 'region_1279.0', 'region_1280.0', 'region_1281.0', 'region_1282.0', 'region_1283.0', 'region_1284.0', 'region_1285.0', 'region_1286.0', 'region_1287.0', 'region_1326.0', 'region_1327.0', 'region_1331.0', 'region_1332.0', 'region_1333.0', 'region_1334.0', 'region_1335.0', 'region_1336.0', 'region_1337.0', 'region_1338.0', 'region_1339.0', 'region_1376.0', 'region_1377.0', 'region_1378.0', 'region_1380.0', 'region_1382.0', 'region_1383.0', 'region_1384.0', 'region_1385.0', 'region_1386.0', 'region_1387.0', 'region_1388.0', 'region_1389.0', 'region_1390.0', 'region_1426.0', 'region_1431.0', 'region_1434.0', 'region_1435.0', 'region_1436.0', 'region_1437.0', 'region_1438.0', 'region_1439.0', 'region_1441.0', 'region_1442.0', 'region_1480.0', 'region_1482.0', 'region_1483.0', 'region_1530.0', 'region_1532.0', 'region_1533.0', 'region_1580.0', 'region_1630.0', 'region_1684.0', 'region_1733.0', 'region_1734.0', 'region_1783.0', 'region_2068.0', 'region_2069.0', 'region_2118.0', 'region_2119.0', 'region_2168.0', 'year_2016.0', 'month_1.0', 'month_2.0', 'month_3.0', 'month_4.0', 'month_5.0', 'month_6.0', 'weekday_0.0', 'weekday_1.0', 'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0', 'weekday_6.0', 'hour_0.0', 'hour_1.0', 'hour_2.0', 'hour_3.0', 'hour_4.0', 'hour_5.0', 'hour_6.0', 'hour_7.0', 'hour_8.0', 'hour_9.0', 'hour_10.0', 'hour_11.0', 'hour_12.0', 'hour_13.0', 'hour_14.0', 'hour_15.0', 'hour_16.0', 'hour_17.0', 'hour_18.0', 'hour_19.0', 'hour_20.0', 'hour_21.0', 'hour_22.0', 'hour_23.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_drop5_num = ['dt_hour']+cols_y+cols_main_cat + cols_polinom #!!! полином здесь не мое изобретение\n",
    "X_train5_num = data5.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[:train_endtime] #cols_main_num\n",
    "X_tune5_num  = data5.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test5_num  = data5.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[test_begtime:test_endtime]\n",
    "\n",
    "cols_drop5_cat = ['dt_hour']+cols_y+cols_main_num + cols_polinom #!!! полином здесь не мое изобретение\n",
    "X_train5_cat = data5.drop(['dt_hour']+cols_y+cols_main_num, axis=1)[:train_endtime] #cols_main_num\n",
    "X_tune5_cat  = data5.drop(['dt_hour']+cols_y+cols_main_num, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test5_cat  = data5.drop(['dt_hour']+cols_y+cols_main_num, axis=1)[test_begtime:test_endtime]\n",
    "\n",
    "y_train5, y_tune5, y_test5 = [], [], []\n",
    "for col in cols_y:\n",
    "    y_train5.append(data5[[col]][:train_endtime])\n",
    "    y_tune5.append(data5[[col]][tune_begtime:tune_endtime])\n",
    "    y_test5.append(data5[[col]][test_begtime:test_endtime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ridge Regression  (5-ая неделя, с числовыми значениями)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rigde Numerical 0.9367453201674699 31.78393749453423\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.Ridge(alpha=250000) \n",
    "lr.fit(X_train5_num,y_train5[0])\n",
    "lr_pred = lr.predict(X_tune5_num)\n",
    "print 'Rigde Numerical', lr5.score(X_tune5_num,y_tune5[0]), metrics.mean_absolute_error(y_tune5[0], lr.predict(X_tune5_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ridge Regression  (5-ая неделя, с категориальными значениями)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rigde Categorial 0.9364733433747702 31.81899723631606\n"
     ]
    }
   ],
   "source": [
    "lr_cat = linear_model.Ridge(alpha=250000) \n",
    "lr_cat.fit(X_train5_cat,y_train5[0])\n",
    "lr_cat_pred = lr_cat.predict(X_tune5_cat)\n",
    "print 'Rigde Categorial', lr_cat.score(X_tune5_cat,\n",
    "                                       y_tune5[0]), metrics.mean_absolute_error(y_tune5[0], lr_cat.predict(X_tune5_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression  (6 неделя, с доп. признаками 1-й набор (RatecodeID, VendorID, payment_type и др.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Оставим числовые признаки от прошлой недели, особой разницы на 5-й неделе не было, а признаков значительно меньше.\n",
    "cols_drop6 = ['dt_hour']+cols_y+cols_main_cat #!!! полином здесь не мое изобретение\n",
    "X_train6 = data6.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[:train_endtime] #cols_main_num\n",
    "X_tune6  = data6.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test6  = data6.drop(['dt_hour']+cols_y+cols_main_cat, axis=1)[test_begtime:test_endtime]\n",
    "\n",
    "y_train6, y_tune6, y_test6 = [], [], []\n",
    "for col in cols_y:\n",
    "    y_train6.append(data6[[col]][:train_endtime])\n",
    "    y_tune6.append(data6[[col]][tune_begtime:tune_endtime])\n",
    "    y_test6.append(data6[[col]][test_begtime:test_endtime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:112: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.637915e-20\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (property set 6.1) 0.970413275301081 21.7651486542025\n"
     ]
    }
   ],
   "source": [
    "lr6 = linear_model.Ridge(alpha=250000) \n",
    "lr6.fit(X_train6,y_train6[0])\n",
    "lr6_pred = lr6.predict(X_tune6)\n",
    "\n",
    "print 'Ridge (property set 6.1)', lr6.score(X_tune6,y_tune6[0]), metrics.mean_absolute_error(y_tune6[0], lr6.predict(X_tune6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Во-вторых__, чтобы улучшить качество прогнозов в аномальные периоды, вы можете найти информацию о потенциально влияющих на количество поездок событиях, таких, как государственные праздники. Проанализируйте, как именно поведение пассажиров меняется во время этих событий, и создайте признаки, отражающие эти изменения. Как показывает наш опыт, правильный учёт праздничных дней часто позволяет существенно уменьшить среднюю ошибку прогноза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/holidays/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data6['date'] = pd.to_datetime(data6.dt_hour.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_holidays = holidays.UnitedStates(state='NY', years=2016)  # or holidays.US(), or holidays.CountryHoliday('US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holidays.UnitedStates"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(us_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hday = pd.DataFrame(us_holidays.items(), columns=['date','hday_name'])\n",
    "df_hday['date'] = pd.to_datetime(df_hday.date)\n",
    "df_hday['hday'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hday.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hday.drop(df_hday[df_hday.date > data6.date.max()].index, inplace=True)\n",
    "df_hday.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hday['hday_label'] = df_hday.hday_name.map(lambda x:x[:5]+'..'+x[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5L,) (5L,)\n"
     ]
    }
   ],
   "source": [
    "print df_hday.hday_label.unique().shape, df_hday.hday_name.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hday_name</th>\n",
       "      <th>hday</th>\n",
       "      <th>hday_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-12</td>\n",
       "      <td>Lincoln's Birthday</td>\n",
       "      <td>1</td>\n",
       "      <td>Linco..thday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>Martin Luther King, Jr. Day</td>\n",
       "      <td>1</td>\n",
       "      <td>Marti... Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-05-30</td>\n",
       "      <td>Memorial Day</td>\n",
       "      <td>1</td>\n",
       "      <td>Memor..l Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>1</td>\n",
       "      <td>New Y..s Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>Washington's Birthday, Susan B. Anthony Day</td>\n",
       "      <td>1</td>\n",
       "      <td>Washi..y Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                    hday_name  hday    hday_label\n",
       "0  2016-02-12                           Lincoln's Birthday     1  Linco..thday\n",
       "9  2016-01-18                  Martin Luther King, Jr. Day     1  Marti... Day\n",
       "12 2016-05-30                                 Memorial Day     1  Memor..l Day\n",
       "6  2016-01-01                               New Year's Day     1  New Y..s Day\n",
       "2  2016-02-15  Washington's Birthday, Susan B. Anthony Day     1  Washi..y Day"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hday.sort_values(by=['hday_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns] datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print data6.date.dtype, df_hday.date.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data6['holiday'] = np.array(pd.merge(data6, df_hday, how = 'left', on = ['date']).hday.fillna(0), int)\n",
    "data6['hday_lbl'] = pd.merge(data6, df_hday, how = 'left', on = ['date']).hday_label.fillna(0), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6['hday_lbl'] = list(pd.merge(data6, df_hday, how = 'left', on = ['date']).hday_label.fillna('General'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6.drop('hday_lbl', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6['hday'] = list(pd.merge(data6, df_hday, how = 'left', on = ['date']).hday_label.fillna('UsualDay'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    432684\n",
       "1     12138\n",
       "Name: holiday, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data6.holiday.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_hday_cat = ['hday_' + x for x in list(data6.hday.value_counts().index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hday_UsualDay</th>\n",
       "      <th>hday_Linco..thday</th>\n",
       "      <th>hday_Marti... Day</th>\n",
       "      <th>hday_Washi..y Day</th>\n",
       "      <th>hday_Memor..l Day</th>\n",
       "      <th>hday_New Y..s Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hday_UsualDay  hday_Linco..thday  hday_Marti... Day  \\\n",
       "2016-01-01 01:00:00              0                  0                  0   \n",
       "2016-01-01 02:00:00              0                  0                  0   \n",
       "\n",
       "                     hday_Washi..y Day  hday_Memor..l Day  hday_New Y..s Day  \n",
       "2016-01-01 01:00:00                  0                  0                  1  \n",
       "2016-01-01 02:00:00                  0                  0                  1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_hday_cat[cols_hday_cat][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hday_UsualDay', 'hday_Linco..thday', 'hday_Marti... Day', 'hday_Washi..y Day', 'hday_Memor..l Day', 'hday_New Y..s Day']\n"
     ]
    }
   ],
   "source": [
    "print cols_hday_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6 = pd.get_dummies(data6, columns=['hday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hday_UsualDay</th>\n",
       "      <th>hday_Linco..thday</th>\n",
       "      <th>hday_Marti... Day</th>\n",
       "      <th>hday_Washi..y Day</th>\n",
       "      <th>hday_Memor..l Day</th>\n",
       "      <th>hday_New Y..s Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hday_UsualDay  hday_Linco..thday  hday_Marti... Day  \\\n",
       "2016-01-01 01:00:00              0                  0                  0   \n",
       "2016-01-01 02:00:00              0                  0                  0   \n",
       "\n",
       "                     hday_Washi..y Day  hday_Memor..l Day  hday_New Y..s Day  \n",
       "2016-01-01 01:00:00                  0                  0                  1  \n",
       "2016-01-01 02:00:00                  0                  0                  1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6[cols_hday_cat][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    432684\n",
       "1     12138\n",
       "Name: holiday, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6.holiday.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Оставим числовые признаки от прошлой недели, особой разницы на 5-й неделе не было, а признаков значительно меньше.\n",
    "cols_drop62 = ['dt_hour', 'date']+cols_y+cols_main_cat #!!! полином здесь не мое изобретение\n",
    "X_train62 = data6.drop(cols_drop62, axis=1)[:train_endtime] #cols_main_num\n",
    "X_tune62  = data6.drop(cols_drop62, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test62  = data6.drop(cols_drop62, axis=1)[test_begtime:test_endtime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:112: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.588551e-20\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Holidays (6.2) 0.9704145132166917 21.761771117389504\n"
     ]
    }
   ],
   "source": [
    "lr62 = linear_model.Ridge(alpha=250000) \n",
    "lr62.fit(X_train62,y_train6[0])\n",
    "lr62_pred = lr62.predict(X_tune62)\n",
    "print 'Ridge Holidays (6.2)', lr62.score(X_tune62,y_tune6[0]), metrics.mean_absolute_error(y_tune6[0], lr62.predict(X_tune62))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Увы, улчшения от праздников не получилось__, скорее всего это можно объяснить тем, что у меня всего один год в выборке, и вряд ли этот признак здесь полезен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В-третьих, можно использовать признаки, связанные с географией. Например, скорее всего, суммарное количество поездок, совершаемых из географической зоны, пропорционально площади этой зоны. Для зон, прилегающих к аэропорту, может быть характерен специфический паттерн дневной сезонности, связанный с тем, что спрос на такси будет повышаться в те часы, когда общественный транспорт перестаёт работать. В деловом центре максимальное количество поездок будет приходиться на начало и окончание рабочего дня, на Бродвее — на время начала и окончания спектаклей. Все эти идеи не обязательно верны, мы приводим их здесь только для того, чтобы продемонстрировать принцип рассуждений. Ещё один пример географического признака: можно попробовать добавить идентификатор боро, который можно найти в файле https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv. Кроме того, нам кажется перспективным использование в качестве фактора количества поездок, совершённых за прошлый час/день и т. д. из соседних географических зон, или количества поездок, совершённых за прошлый час/день в текущую географическую зону."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поездки соседних районов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>west</th>\n",
       "      <th>east</th>\n",
       "      <th>south</th>\n",
       "      <th>north</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-74.25559</td>\n",
       "      <td>-74.244478</td>\n",
       "      <td>40.496120</td>\n",
       "      <td>40.504508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-74.25559</td>\n",
       "      <td>-74.244478</td>\n",
       "      <td>40.504508</td>\n",
       "      <td>40.512896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region      west       east      south      north\n",
       "0       1 -74.25559 -74.244478  40.496120  40.504508\n",
       "1       2 -74.25559 -74.244478  40.504508  40.512896"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загружаем районы\n",
    "dfreg = pd.read_csv('regions.csv', sep=';') \n",
    "dfreg[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#данные в виде двумерного массива (строки-y, стобцы-x)\n",
    "rm = np.array(dfreg.sort_values('region').region).reshape([50,50]) \n",
    "rm_line = np.zeros([50,1], int)\n",
    "#для определения соседних районов сместим матрицу влево-вправо-верх-вниз, тогда на преждней  позиции района окажется сосед\n",
    "rm_r    = np.hstack([rm[:, 1:], rm_line])     #сосед справа\n",
    "rm_l    = np.hstack([rm_line, rm[:, :-1]])    #сосед слева\n",
    "rm_dn   = np.vstack([rm[1:, :], rm_line.T])   #сосед снизу\n",
    "rm_up   = np.vstack([rm_line.T, rm[:-1, :]])  #сосед сверху \n",
    "rm_ldn  = np.hstack([rm_line, rm_dn[:, :-1]]) #сосед слева-снизу\n",
    "rm_lup  = np.hstack([rm_line, rm_up[:, :-1]]) #сосед слева-сверху\n",
    "rm_rdn  = np.hstack([rm_dn[:, 1:], rm_line])  #сосед справа-снизу\n",
    "rm_rup  = np.hstack([rm_up[:, 1:], rm_line])  #сосед справа-сверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_reg_next = {}\n",
    "for i in range(rm.shape[0]):\n",
    "    for j in range(rm.shape[1]):\n",
    "        reg_next0 = [rm_r[i, j] , rm_l[i, j] , rm_dn[i,j], rm_up[i,j], rm_ldn[i,j], rm_lup[i,j], rm_rdn[i,j], rm_rup[i,j]]\n",
    "        reg_next = [x for x in reg_next0 if x != 0]\n",
    "        if rm[i, j] in list(actual_regs.region):\n",
    "            dic_reg_next[rm[i, j]] = reg_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic_reg_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febcaf4dd0fb448ea01f0aff270e81de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data6['near_trips'] = 0\n",
    "for r in tqdm_notebook(dic_reg_next.items()):\n",
    "    reg_num  = r[0]\n",
    "    reg_near = r[1]\n",
    "    np_near = np.array(data6[data6.region.isin(reg_near)].groupby(['dt_hour'], as_index=False).trips.sum().trips, int)\n",
    "    data6.loc[data6.region == reg_num, 'near_trips'] = np.array(np_near, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data6['prev_date'] = data6.dt_hour - pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data6.loc[:, 'prev_near_trips'] = np.array(pd.merge(data6, data6, how='left', left_on=['region', 'prev_date'], \n",
    "                                                   right_on=['region','dt_hour'])['near_trips_y'].fillna(0), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Оставим числовые признаки от прошлой недели, особой разницы на 5-й неделе не было, а признаков значительно меньше.\n",
    "cols_drop63 = ['dt_hour', 'date','prev_date']+cols_y+cols_main_cat #!!! полином здесь не мое изобретение\n",
    "X_train63 = data6.drop(cols_drop63, axis=1)[:train_endtime] #cols_main_num\n",
    "X_tune63  = data6.drop(cols_drop63, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test63  = data6.drop(cols_drop63, axis=1)[test_begtime:test_endtime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Neartrips (6.3) 0.9714671386992391 21.583327071471732\n"
     ]
    }
   ],
   "source": [
    "lr63 = linear_model.Ridge(alpha=250000) \n",
    "lr63.fit(X_train63,y_train6[0])\n",
    "lr63_pred = lr63.predict(X_tune63)\n",
    "print 'Ridge Neartrips (6.3)', lr63.score(X_tune63,y_tune6[0]), metrics.mean_absolute_error(y_tune6[0], lr63.predict(X_tune63))\n",
    "# 0.970413275301081 21.7651486542025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшение (поездки соседних районов): 0.181821582731\n"
     ]
    }
   ],
   "source": [
    "print 'Улучшение (поездки соседних районов):', 21.7651486542025 - 21.583327071471732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стало несколько лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аэропорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_airport = dfreg[(dfreg.west < -73.7781391) & \n",
    "                    (dfreg.east > -73.7781391) & (dfreg.south < 40.6413111) & (dfreg.north > 40.6413111)].region.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2118\n"
     ]
    }
   ],
   "source": [
    "print reg_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2116</td>\n",
       "      <td>2117</td>\n",
       "      <td>2118</td>\n",
       "      <td>2119</td>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2166</td>\n",
       "      <td>2167</td>\n",
       "      <td>2168</td>\n",
       "      <td>2169</td>\n",
       "      <td>2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2216</td>\n",
       "      <td>2217</td>\n",
       "      <td>2218</td>\n",
       "      <td>2219</td>\n",
       "      <td>2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2266</td>\n",
       "      <td>2267</td>\n",
       "      <td>2268</td>\n",
       "      <td>2269</td>\n",
       "      <td>2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2316</td>\n",
       "      <td>2317</td>\n",
       "      <td>2318</td>\n",
       "      <td>2319</td>\n",
       "      <td>2320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2366</td>\n",
       "      <td>2367</td>\n",
       "      <td>2368</td>\n",
       "      <td>2369</td>\n",
       "      <td>2370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      15    16    17    18    19\n",
       "42  2116  2117  2118  2119  2120\n",
       "43  2166  2167  2168  2169  2170\n",
       "44  2216  2217  2218  2219  2220\n",
       "45  2266  2267  2268  2269  2270\n",
       "46  2316  2317  2318  2319  2320\n",
       "47  2366  2367  2368  2369  2370"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rm).iloc[42:48, 15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Район аэропорта\n",
    "data6['airport_area'] = 0\n",
    "data6.loc[data6.region.isin(dic_reg_next[reg_airport]), 'airport_area'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Время работы групповых шатлов: 5.30 23:30\n",
    "data6['airport_shuttle'] = 0\n",
    "data6.loc[(data6['airport_area']==1) & (data6.hour >= 6) & (data6.hour <= 23), 'airport_shuttle'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_drop64 = ['dt_hour', 'date','prev_date']+cols_y+cols_main_cat\n",
    "X_train64 = data6.drop(cols_drop64, axis=1)[:train_endtime]\n",
    "X_tune64  = data6.drop(cols_drop64, axis=1)[tune_begtime:tune_endtime]\n",
    "X_test64  = data6.drop(cols_drop64, axis=1)[test_begtime:test_endtime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Neartrips (6.4) 0.9714671959181759 21.58316822352148\n"
     ]
    }
   ],
   "source": [
    "lr64 = linear_model.Ridge(alpha=250000) \n",
    "lr64.fit(X_train64,y_train6[0])\n",
    "lr64_pred = lr64.predict(X_tune64)\n",
    "print 'Ridge Neartrips (6.4)', lr64.score(X_tune64,y_tune6[0]), metrics.mean_absolute_error(y_tune6[0], lr64.predict(X_tune64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшение (выделена зона аэропорта): 5.06615385341e-05\n"
     ]
    }
   ],
   "source": [
    "print 'Улучшение (выделена зона аэропорта):', 21.583327071471732-21.583276409933198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стало еще немного лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлом задании я остановился на модели гремневой регрессии. Однако дополнительные признаки, улучшив результат в абсолбте, все-таки не сделали его достаточно хорошим. Попроюуем другие модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosting Regressor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bgr = ensemble.GradientBoostingRegressor().fit(X_train6,y_train6[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.509643306020326\n"
     ]
    }
   ],
   "source": [
    "print(metrics.mean_absolute_error(y_tune6[0], bgr.predict(X_tune6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Долго обучается, но результат значительно лучше!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf64 = ensemble.RandomForestRegressor() \n",
    "rf64.fit(X_train64,y_train6[0])\n",
    "rf64_pred = rf64.predict(X_tune64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.600332988405103\n"
     ]
    }
   ],
   "source": [
    "print metrics.mean_absolute_error(y_tune6[0], rf64_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Обучается тоже долго, но результат хороший! Оставим эту модель__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1b8d266ee7462da8e559358f1c0d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка прогноза: 33.85872499567409\n"
     ]
    }
   ],
   "source": [
    "lr64_list = []\n",
    "Q_list = []\n",
    "for i in tqdm_notebook(range(6)):\n",
    "    lr64 = linear_model.Ridge(alpha=250000) \n",
    "    lr64.fit(X_train64,y_train6[i])\n",
    "    lr64_pred = lr64.predict(X_tune64)\n",
    "    lr64_list.append(lr64)\n",
    "    Q_list.append(metrics.mean_absolute_error(y_tune6[i], lr64_pred))\n",
    "\n",
    "Q_tune = sum(Q_list) / 6\n",
    "print 'Ошибка прогноза:', Q_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b031b53638fc496fae50f776691e30d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n",
      "C:\\Users\\anna8\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка прогноза: 20.297729664712076\n"
     ]
    }
   ],
   "source": [
    "rf64_list = []\n",
    "rfQ_list = []\n",
    "for i in tqdm_notebook(range(6)):\n",
    "    rf64 = ensemble.RandomForestRegressor() \n",
    "    rf64.fit(X_train64.values,y_train6[i].values)\n",
    "    rf64_pred = rf64.predict(X_tune64)\n",
    "    rf64_list.append(rf64)\n",
    "    rfQ_list.append(metrics.mean_absolute_error(y_tune6[i], rf64_pred))\n",
    "\n",
    "rfQ_tune = sum(rfQ_list) / 6\n",
    "print 'Ошибка прогноза:', rfQ_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c377f2e8e64fd1b01a878634b5d348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075_2016-05-31_23_1,[11.80525603]\n",
      "1075_2016-06-01_0_1,[14.2365185]\n",
      "1075_2016-06-01_1_1,[0.64724212]\n",
      "1075_2016-06-01_2_1,[-3.92622262]\n",
      "1075_2016-06-01_3_1,[1.84472852]\n",
      "1075_2016-06-01_4_1,[4.47648728]\n",
      "1075_2016-06-01_5_1,[14.49545111]\n",
      "1075_2016-06-01_6_1,[43.9358622]\n",
      "1075_2016-06-01_7_1,[53.98920525]\n",
      "1075_2016-06-01_8_1,[63.52353031]\n"
     ]
    }
   ],
   "source": [
    "fin_pred_list = []\n",
    "fin_kaggle = []\n",
    "X_regions =  X_test64.region.values \n",
    "for i in tqdm_notebook(range(6)):\n",
    "    fin_pred = rf64_list[i].predict(X_test64)\n",
    "    fin_pred_list.append(fin_pred)\n",
    "    for j, t in enumerate(X_test64.index):\n",
    "        region = X_regions[j]\n",
    "        idx = '_'.join([str(int(region)),str(t.date()),str(t.hour),str(i+1)])\n",
    "        y = str((fin_pred[j]))\n",
    "        answer = ','.join([idx, y])\n",
    "        fin_kaggle.append(answer)\n",
    "\n",
    "print('\\n'.join(predict_kaggle[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e28ef83a7784e1fa78524ef47b1af80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.929226884226885\n"
     ]
    }
   ],
   "source": [
    "Qs = []\n",
    "for i in tqdm_notebook(range(6)):\n",
    "    fin_pred = fin_pred_list[i] \n",
    "    Qs.append(metrics.mean_absolute_error(y_test6[i], fin_pred))\n",
    "\n",
    "print sum(Qs) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data6_path + 'Alexey_Bebchik_k6w6_answer3.csv','w') as f:\n",
    "    f.write('id,y\\n')\n",
    "    f.write('\\n'.join(fin_kaggle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВЫВОД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе добавлены:\n",
    "1 дополнительные признаки из сырых данных (тариф,вендор и др.);  \n",
    "2 внешние признаки праздников;  \n",
    "3 географические признаки (данные соседних районов, район аэропорта).  \n",
    "\n",
    "Качество прогноза значительно выросло после добавления первой группы признаков,  еще несколько выросло третьей группе. Праздники должного эффекта не дали. Качество обучение проверялось на гребневой регрессии, подобранной в прошлом задании. \n",
    "\n",
    "Далее было попробованы еще две модели - градиентный бустинг и случайный лес. Случайный лес справился лучше, на нем и выполнен финальный расчет проноза.\n",
    "\n",
    "Файл ответа загружен на Kaggle: AlexModern, 63 место (score __19.92921__) Приложен скриншот:   \n",
    "\n",
    "__Alexey_Bebchik_Kaggle_Leaderbord_Week6(AlexModern)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b184eec44b2b43518662a1c601f2c8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка прогноза: 45.111395752276906\n"
     ]
    }
   ],
   "source": [
    "Q_list_test = []\n",
    "for i in tqdm_notebook(range(6)):\n",
    "    lr64_pred_test = lr64_list[i].predict(X_test64)\n",
    "    Q_list_test.append(metrics.mean_absolute_error(y_test6[i], lr64_list[i].predict(X_test64)))\n",
    "\n",
    "Q_test = sum(Q_list_test) / 6\n",
    "print 'Ошибка прогноза:', Q_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# формируем идентификатор: Регион_ДатаЧас_ЧасПрозноза\n",
    "pred['save_id'] = pred.region.astype(int).astype(str) + '_'+pred.dt_hour.dt.strftime('%Y-%m-%d_%H')\n",
    "# если в часе даты конца истории встречается 0, например, 03, то надо оставить 3 (если 00, то будет 0)\n",
    "pred['save_id'] = pred.save_id.apply(lambda x:x[:-2]+x[-1:] if x[-2:-1]=='0' else x)\n",
    "# добавляем номер прогнозного часа от 1 до 6\n",
    "pred['save_id'] = pred['save_id'] + '_' + pred.num6.astype(str) #.apply(lambda x:x[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regs = data6.region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1075.,  1076.,  1077.,  1125.,  1126.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trip_len, passenger_count, trip_distance, region2, RatecodeID, payment_type, fare_amount, VendorID\n",
    "\n",
    "\n",
    "trip_len = tpep_dropoff_datetime - tpep_pickup_datetime средняя длительность поездок;      \n",
    "passenger_count среднее количество пассажиров;      \n",
    "trip_distance среднее расстояние по счётчику;      \n",
    "region2 = dropoff_longitude+dropoff_latitude доли географических зон, в которые совершаются поездки;      \n",
    "RatecodeID доли поездок, совершаемых по тарифам каждого из типов;      \n",
    "payment_type доли способов оплаты поездок;      \n",
    "fare_amount средняя стоимость поездок;      \n",
    "VendorID доли провайдеров данных.    \n",
    "\n",
    "ndex([u'VendorID', u'tpep_pickup_datetime', u'tpep_dropoff_datetime',  \n",
    "       u'passenger_count', u'trip_distance', u'pickup_longitude',\n",
    "       u'pickup_latitude', u'RatecodeID', u'store_and_fwd_flag',\n",
    "       u'dropoff_longitude', u'dropoff_latitude', u'payment_type',\n",
    "       u'fare_amount', u'extra', u'mta_tax', u'tip_amount', u'tolls_amount',\n",
    "       u'improvement_surcharge', u'total_amount'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_regs = pd.read_csv(data6_path + 'YTw6_reg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Загрузим данные, ограничив их по району, количеству пассажиров и продолжительности поездки\n",
    "data_cols = ['tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count', 'trip_distance','pickup_longitude',\n",
    "             'pickup_latitude'] + ['dropoff_longitude', 'dropoff_latitude', 'RatecodeID' , 'payment_type', \n",
    "                                   'fare_amount', 'total_amount', 'VendorID']\n",
    "cols_cut = ['dt_hour', 'region', 'region2', 'passenger_count', 'trip_distance', \n",
    "             'RatecodeID', 'payment_type', 'fare_amount', 'total_amount', 'VendorID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read yellow_tripdata_2016-01.csv 16:33\n",
      "create del index 16:34\n",
      "binned_statistic_2d  16:35\n",
      "binned_statistic_2d 2 16:35\n",
      "drop regs less 5\n",
      "save file\n",
      "read yellow_tripdata_2016-02.csv 16:37\n",
      "create del index 16:39\n",
      "binned_statistic_2d  16:39\n",
      "binned_statistic_2d 2 16:39\n",
      "drop regs less 5\n",
      "save file\n",
      "read yellow_tripdata_2016-03.csv 16:41\n",
      "create del index 16:42\n",
      "binned_statistic_2d  16:42\n",
      "binned_statistic_2d 2 16:43\n",
      "drop regs less 5\n",
      "save file\n",
      "read yellow_tripdata_2016-04.csv 16:44\n",
      "create del index 16:46\n",
      "binned_statistic_2d  16:46\n",
      "binned_statistic_2d 2 16:46\n",
      "drop regs less 5\n",
      "save file\n",
      "read yellow_tripdata_2016-05.csv 16:48\n",
      "create del index 16:49\n",
      "binned_statistic_2d  16:49\n",
      "binned_statistic_2d 2 16:50\n",
      "drop regs less 5\n",
      "save file\n",
      "read yellow_tripdata_2016-06.csv 16:51\n",
      "create del index 16:52\n",
      "binned_statistic_2d  16:52\n",
      "binned_statistic_2d 2 16:53\n",
      "drop regs less 5\n",
      "save file\n",
      "Wall time: 21min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "save_fname = 'k6w6_data_ext.csv', \n",
    "month_count = 6\n",
    "reg_fname='regions.csv'\n",
    "cut_regLess5 = True\n",
    "save_data=False #True\n",
    "# return_data=True\n",
    "\n",
    "#Координаты квадрата Нью_Йорка\n",
    "west_ny, east_ny, south_ny, north_ny  = -74.25559,-73.700018, 40.49612, 40.91553\n",
    "\n",
    "#Загружаем районы и получаем границы корзин\n",
    "regs = pd.read_csv(data_path + 'regions.csv', sep=';') \n",
    "binx  = sorted(set(regs.west.value_counts().index)  | set(regs.east.value_counts().index ))\n",
    "biny  = sorted(set(regs.south.value_counts().index) | set(regs.north.value_counts().index))\n",
    "\n",
    "# Составим список файлов для обработки\n",
    "files = [data_path + 'yellow_tripdata_2016-' + ('0'+str(x+1))[-2:] + '.csv' for x in range(month_count)] [:] #для отладкаи 5\n",
    "\n",
    "dfmon_list = []\n",
    "for i, f in enumerate(files): #tqdm_notebook(\n",
    "#     sleep(0.1)\n",
    "    print_time('read '+ f)\n",
    "    \n",
    "    dfmon = pd.read_csv(f, sep=',', parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'], usecols=data_cols) \n",
    "    \n",
    "    #print_time('get del_index')\n",
    "    \n",
    "    print_time('create del index')\n",
    "    del_index = dfmon[(dfmon.tpep_pickup_datetime == dfmon.tpep_dropoff_datetime) #Нулевая длительность\n",
    "                | (dfmon['passenger_count' ] == 0)          # поездки с нулевым количеством пассажиров\n",
    "                | (dfmon['trip_distance'   ] == 0)          # поездки с нулевым расстоянием поездки по счётчику\n",
    "                | (dfmon['pickup_longitude'] <  west_ny)    # поездки, не попадающие в прямоугольник Нью-Йорка\n",
    "                | (dfmon['pickup_longitude'] >= east_ny)    \n",
    "                | (dfmon['pickup_latitude' ] <  south_ny)   \n",
    "                | (dfmon['pickup_latitude' ] >= north_ny)  \n",
    "                     ].index\n",
    "    #print_time('drop del_index')\n",
    "    dfmon.drop(del_index, inplace=True)\n",
    "    dfmon['dt_hour'] = dfmon.tpep_pickup_datetime.dt.floor('H') #день и час (.dt.hour - только час)\n",
    "    #длительность поездки\n",
    "    dfmon.loc[:,'trip_len'] = ((dfmon.tpep_dropoff_datetime - dfmon.tpep_pickup_datetime) / np.timedelta64(1, 'm')).astype(int)\n",
    "    \n",
    "    print_time('binned_statistic_2d ')\n",
    "    \n",
    "    #Разобъем данные на квадраты\n",
    "    r_bins = binned_statistic_2d(\n",
    "        x = dfmon.pickup_longitude, \n",
    "        y = dfmon.pickup_latitude, \n",
    "        values = None, \n",
    "        statistic = 'count', \n",
    "        bins      = [binx, biny],\n",
    "        range     = [[-74.25559, -73.70001], [40.49612, 40.91553]], expand_binnumbers = True)\n",
    "    # Вычислим номер региона\n",
    "    bin_regs = np.array([(x-1) * 50 + y for x, y in zip(*r_bins.binnumber)])\n",
    "    #print_time('set regs')\n",
    "    dfmon['region']  = bin_regs\n",
    "\n",
    "    #Разобъем данные на квадраты\n",
    "    print_time('binned_statistic_2d 2')\n",
    "    r_bins2 = binned_statistic_2d(\n",
    "        x = dfmon.dropoff_longitude, \n",
    "        y = dfmon.dropoff_latitude, \n",
    "        values = None, \n",
    "        statistic = 'count', \n",
    "        bins      = [binx, biny],\n",
    "        range     = [[-74.25559, -73.70001], [40.49612, 40.91553]], expand_binnumbers = True)\n",
    "    # Вычислим номер региона\n",
    "    bin_regs2 = np.array([(x-1) * 50 + y for x, y in zip(*r_bins2.binnumber)])\n",
    "    #print_time('set regs')\n",
    "    dfmon['region2']  = bin_regs2\n",
    "    \n",
    "    print 'drop regs less 5'\n",
    "    dfmon = dfmon[dfmon.region.isin(list(actual_regs.region))]\n",
    "    \n",
    "    print 'saveing file'\n",
    "    dfmon[cols_cut].to_csv('cut2_'+ f, index=False, chunksize=100000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month_count = 6\n",
    "cut_files = [data6_path + 'cut2_yellow_tripdata_2016-' + ('0'+str(x+1))[-2:] + '.csv' for x in range(month_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cut2_yellow_tripdata_2016-01.csv 09:10\n",
      "load cut2_yellow_tripdata_2016-02.csv 09:11\n",
      "load cut2_yellow_tripdata_2016-03.csv 09:11\n",
      "load cut2_yellow_tripdata_2016-04.csv 09:11\n",
      "load cut2_yellow_tripdata_2016-05.csv 09:12\n",
      "load cut2_yellow_tripdata_2016-06.csv 09:12\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "dfmon_list = []\n",
    "for i, f in enumerate(cut_files): #tqdm_notebook(\n",
    "    print_time('load '+ f)\n",
    "    dfmon = pd.read_csv(f, sep=',', parse_dates=['dt_hour']) #, usecols=cols_cut) \n",
    "    dfmon_list.append(dfmon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hour</th>\n",
       "      <th>region</th>\n",
       "      <th>region2</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>VendorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1230</td>\n",
       "      <td>1532</td>\n",
       "      <td>2</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1177</td>\n",
       "      <td>1336</td>\n",
       "      <td>2</td>\n",
       "      <td>7.45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dt_hour  region  region2  passenger_count  trip_distance  RatecodeID  \\\n",
       "0 2016-01-01    1230     1532                2           5.52           1   \n",
       "1 2016-01-01    1177     1336                2           7.45           1   \n",
       "\n",
       "   payment_type  fare_amount  total_amount  VendorID  \n",
       "0             2         19.0          20.3         2  \n",
       "1             2         26.0          27.3         2  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmon_list[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0, get_summies 11:28\n",
      "file 0, groupby 11:28\n",
      "file 1, get_summies 11:29\n",
      "file 1, groupby 11:29\n",
      "file 2, get_summies 11:29\n",
      "file 2, groupby 11:30\n",
      "file 3, get_summies 11:30\n",
      "file 3, groupby 11:30\n",
      "file 4, get_summies 11:30\n",
      "file 4, groupby 11:31\n",
      "file 5, get_summies 11:31\n",
      "file 5, groupby 11:32\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "cols_ext_cat  = ['VendorID', 'payment_type', 'RatecodeID']\n",
    "dfm_agr_list  = []\n",
    "cols_agr_list = []\n",
    "for i, dfm in enumerate(dfmon_list):\n",
    "\n",
    "    # Категориальные признаки \n",
    "    cols_main   = list(dfm.columns)\n",
    "    #     sleep(0.1)\n",
    "    print_time('file '+ str(i) + ', get_dummies')\n",
    "    dfm_ext_cat = pd.get_dummies(dfm, columns=cols_ext_cat)\n",
    "\n",
    "    cols_ext  = list(dfm_ext_cat.columns)[len(cols_main)-len(cols_ext_cat):]\n",
    "    print_time('file '+ str(i) + ', groupby')\n",
    "    dfm_agr = dfm_ext_cat.groupby(['dt_hour', 'region'], as_index=False).mean()\n",
    "\n",
    "    dfm_trips = dfm.groupby(['dt_hour', 'region'], as_index=False).passenger_count.count()\n",
    "\n",
    "    dfm_agr.loc[:, 'trips'] = dfm_trips.passenger_count\n",
    "    dfm_agr.loc[:, 'trips'] = np.array(dfm_agr.trips, int)\n",
    "\n",
    "    cols_agr_list.append(cols_ext1)\n",
    "    dfm_agr_list.append(dfm_agr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hour</th>\n",
       "      <th>region</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>1075</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dt_hour  region  passenger_count\n",
       "0 2016-06-01    1075               26"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_trips[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hour</th>\n",
       "      <th>region</th>\n",
       "      <th>region2</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>VendorID_1</th>\n",
       "      <th>VendorID_2</th>\n",
       "      <th>payment_type_1</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_type_4</th>\n",
       "      <th>payment_type_5</th>\n",
       "      <th>RatecodeID_1</th>\n",
       "      <th>RatecodeID_2</th>\n",
       "      <th>RatecodeID_3</th>\n",
       "      <th>RatecodeID_4</th>\n",
       "      <th>RatecodeID_5</th>\n",
       "      <th>RatecodeID_6</th>\n",
       "      <th>RatecodeID_99</th>\n",
       "      <th>trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1075</td>\n",
       "      <td>1203.9</td>\n",
       "      <td>2.0375</td>\n",
       "      <td>3.972625</td>\n",
       "      <td>14.88125</td>\n",
       "      <td>17.590625</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dt_hour  region  region2  passenger_count  trip_distance  fare_amount  \\\n",
       "0 2016-01-01    1075   1203.9           2.0375       3.972625     14.88125   \n",
       "\n",
       "   total_amount  VendorID_1  VendorID_2  payment_type_1  ...    \\\n",
       "0     17.590625      0.4375      0.5625           0.425  ...     \n",
       "\n",
       "   payment_type_4  payment_type_5  RatecodeID_1  RatecodeID_2  RatecodeID_3  \\\n",
       "0             0.0             0.0           1.0           0.0           0.0   \n",
       "\n",
       "   RatecodeID_4  RatecodeID_5  RatecodeID_6  RatecodeID_99  trips  \n",
       "0           0.0           0.0           0.0            0.0     26  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_agr[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm_trips, dfm_agr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_hour</th>\n",
       "      <th>region</th>\n",
       "      <th>region2</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>VendorID_1</th>\n",
       "      <th>VendorID_2</th>\n",
       "      <th>payment_type_1</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_type_3</th>\n",
       "      <th>payment_type_4</th>\n",
       "      <th>payment_type_5</th>\n",
       "      <th>RatecodeID_1</th>\n",
       "      <th>RatecodeID_2</th>\n",
       "      <th>RatecodeID_3</th>\n",
       "      <th>RatecodeID_4</th>\n",
       "      <th>RatecodeID_5</th>\n",
       "      <th>RatecodeID_6</th>\n",
       "      <th>RatecodeID_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1075</td>\n",
       "      <td>967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>29.5</td>\n",
       "      <td>32.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1075</td>\n",
       "      <td>1076</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dt_hour  region  region2  passenger_count  trip_distance  fare_amount  \\\n",
       "0 2016-01-01    1075      967              1.0          10.00         29.5   \n",
       "1 2016-01-01    1075     1076              1.2           0.57          6.0   \n",
       "\n",
       "   total_amount  VendorID_1  VendorID_2  payment_type_1      ...        \\\n",
       "0         32.80         1.0         0.0             1.0      ...         \n",
       "1          8.31         0.6         0.4             0.6      ...         \n",
       "\n",
       "   payment_type_3  payment_type_4  payment_type_5  RatecodeID_1  RatecodeID_2  \\\n",
       "0             0.0             0.0             0.0           1.0           0.0   \n",
       "1             0.0             0.0             0.0           1.0           0.0   \n",
       "\n",
       "   RatecodeID_3  RatecodeID_4  RatecodeID_5  RatecodeID_6  RatecodeID_99  \n",
       "0           0.0           0.0           0.0           0.0            0.0  \n",
       "1           0.0           0.0           0.0           0.0            0.0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_agr_list[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "for i, dfm_agr in enumerate(dfm_agr_list):\n",
    "    print_time('k6w4_data_ext_' + str(i))\n",
    "    dfm_agr.to_csv('k6w4_data_ext_' + str(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#все регионы\n",
    "regions  = pd.DataFrame(actual_regs.region, columns=['region'])\n",
    "regions['key'] = 0 #для cross join в merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "data_hr_list = []\n",
    "\n",
    "for i, dfm_agr in enumerate(dfm_agr_list):\n",
    "    \n",
    "    print_time('hr' + str(i) +  ' ' +  str(dfm_agr.shape))\n",
    "    \n",
    "    #все дни и часы за весь период в данных\n",
    "    start_date  = dfm_agr.dt_hour.min() #дата начала данных\n",
    "    end_date    = dfm_agr.dt_hour.max() #дата окончания данных\n",
    "    dt_hours = pd.DataFrame(pd.date_range(start_date, end_date, freq='H').values, columns=['dt_hour'])\n",
    "    dt_hours['key'] = 0 #для cross join в merge \n",
    "\n",
    "    #все дни и часы для каждого региона (cross join)\n",
    "    data_hr = pd.merge(regions, dt_hours, how='outer')[['dt_hour','region']]\n",
    "\n",
    "    print_time('update hr' + str(i))\n",
    "    \n",
    "    #добавим столбец с количеством поездок для каждой пары с помощью left join, заполнив NaN нулями\n",
    "    for c in list(dfm_agr.columns[3:]):\n",
    "        data_hr[c] = data_hr.merge(dfm_agr, how='left', on = ['dt_hour', 'region']).fillna(0)[c]\n",
    "    \n",
    "    print_time('save hr' + str(i) + str(data_hr.shape))\n",
    "    \n",
    "    data_hr_list.append(data_hr)\n",
    "    \n",
    "    dfm_agr.to_csv(data4_path + 'YTw4_data_ext_hr' + str(i), index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RatecodeID_1</th>\n",
       "      <th>RatecodeID_2</th>\n",
       "      <th>RatecodeID_3</th>\n",
       "      <th>RatecodeID_4</th>\n",
       "      <th>RatecodeID_5</th>\n",
       "      <th>RatecodeID_6</th>\n",
       "      <th>RatecodeID_99</th>\n",
       "      <th>VendorID_1</th>\n",
       "      <th>VendorID_2</th>\n",
       "      <th>dt_hour</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>payment_type_1</th>\n",
       "      <th>payment_type_2</th>\n",
       "      <th>payment_type_3</th>\n",
       "      <th>payment_type_4</th>\n",
       "      <th>payment_type_5</th>\n",
       "      <th>region</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>14.88125</td>\n",
       "      <td>2.0375</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1075</td>\n",
       "      <td>17.590625</td>\n",
       "      <td>3.972625</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RatecodeID_1  RatecodeID_2  RatecodeID_3  RatecodeID_4  RatecodeID_5  \\\n",
       "0           1.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   RatecodeID_6  RatecodeID_99  VendorID_1  VendorID_2    dt_hour  \\\n",
       "0           0.0            0.0      0.4375      0.5625 2016-01-01   \n",
       "\n",
       "   fare_amount  passenger_count  payment_type_1  payment_type_2  \\\n",
       "0     14.88125           2.0375           0.425           0.575   \n",
       "\n",
       "   payment_type_3  payment_type_4  payment_type_5  region  total_amount  \\\n",
       "0             0.0             0.0             0.0    1075     17.590625   \n",
       "\n",
       "   trip_distance  trips  \n",
       "0       3.972625   80.0  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hr_all[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_hr_all = pd.concat(data_hr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((445536, 21), (445536, 3))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hr_all.shape, data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_hr_all.to_csv('k6w4_data_ext_hr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Блок определения пользовательских функций__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Готовим данные и записываем их в файл (примерно 6 минут да 2016 год)\n",
    "def load_prepare_save_data(save_fname, month_count, reg_fname= data_path + 'regions.csv', \n",
    "                           cut_regLess5 = True, save_data=True, return_data=True):\n",
    "    \n",
    "    #Координаты квадрата Нью_Йорка\n",
    "    west_ny, east_ny, south_ny, north_ny  = -74.25559,-73.700018, 40.49612, 40.91553\n",
    "    \n",
    "    #Загружаем районы и получаем границы корзин\n",
    "    regs = pd.read_csv(reg_fname, sep=';') \n",
    "    binx  = sorted(set(regs.west.value_counts().index)  | set(regs.east.value_counts().index ))\n",
    "    biny  = sorted(set(regs.south.value_counts().index) | set(regs.north.value_counts().index))\n",
    "    \n",
    "    \n",
    "    # Составим список файлов для обработки\n",
    "    files = [data_path + 'yellow_tripdata_2016-' + ('0'+str(x+1))[-2:] + '.csv' for x in range(month_count)]# [:1] для отладкаи 5\n",
    "\n",
    "    #Загрузим данные, ограничив их по району, количеству пассажиров и продолжительности поездки\n",
    "    data_cols = ['tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count',\n",
    "                 'trip_distance','pickup_longitude','pickup_latitude'] \n",
    "\n",
    "    data_cols = ['tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count', 'trip_distance','pickup_longitude',\n",
    "                 'pickup_latitude'] + ['dropoff_longitude', 'dropoff_latitude', 'RatecodeID' , 'payment_type', \n",
    "                                       'fare_amount', 'total_amount', 'VendorID']\n",
    "    dfmon_list = []\n",
    "    for f in files:\n",
    "        print_time('read '+ f)\n",
    "        dfmon = pd.read_csv(f, sep=',', parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'], usecols=data_cols) \n",
    "        #print_time('get del_index')\n",
    "        del_index = dfmon[(dfmon.tpep_pickup_datetime == dfmon.tpep_dropoff_datetime) #Нулевая длительность\n",
    "                    | (dfmon['passenger_count' ] == 0)          # поездки с нулевым количеством пассажиров\n",
    "                    | (dfmon['trip_distance'   ] == 0)          # поездки с нулевым расстоянием поездки по счётчику\n",
    "                    | (dfmon['pickup_longitude'] <  west_ny)    # поездки, не попадающие в прямоугольник Нью-Йорка\n",
    "                    | (dfmon['pickup_longitude'] >= east_ny)    \n",
    "                    | (dfmon['pickup_latitude' ] <  south_ny)   \n",
    "                    | (dfmon['pickup_latitude' ] >= north_ny)  \n",
    "                         ].index\n",
    "        #print_time('drop del_index')\n",
    "        dfmon.drop(del_index, inplace=True)\n",
    "        dfmon['dt_hour'] = dfmon.tpep_pickup_datetime.dt.floor('H') #день и час (.dt.hour - только час)\n",
    "        #print_time('calc reg bins')\n",
    "        #Разобъем данные на квадраты\n",
    "        r_bins = binned_statistic_2d(\n",
    "            x = dfmon.pickup_longitude, \n",
    "            y = dfmon.pickup_latitude, \n",
    "            values = None, \n",
    "            statistic = 'count', \n",
    "            bins      = [binx, biny],\n",
    "            range     = [[-74.25559, -73.70001], [40.49612, 40.91553]], expand_binnumbers = True)\n",
    "        # Вычислим номер региона\n",
    "        bin_regs = np.array([(x-1) * 50 + y for x, y in zip(*r_bins.binnumber)])\n",
    "        #print_time('set regs')\n",
    "        dfmon['region']  = bin_regs\n",
    "        #print_time('aggregate')\n",
    "        dfmon_agr = dfmon.groupby(['dt_hour', 'region'], as_index=False).pickup_longitude.count()\n",
    "        dfmon_agr.columns = ['dt_hour', 'region', 'trips']\n",
    "        dfmon_list.append(dfmon_agr)\n",
    "\n",
    "    data_trips = pd.concat(dfmon_list) #только совершенные поездки в регионах\n",
    "\n",
    "    #---- надо добавить нули для регионов и часов, в которых не было поездок\n",
    "\n",
    "    #все регионы\n",
    "    regions  = pd.DataFrame(regs.region, columns=['region'])\n",
    "    regions['key'] = 0 #для cross join в merge \n",
    "\n",
    "    #все дни и часы за весь период в данных\n",
    "    start_date  = data_trips.dt_hour.min() #дата начала данных\n",
    "    end_date    = data_trips.dt_hour.max() #дата окончания данных\n",
    "    dt_hours = pd.DataFrame(pd.date_range(start_date, end_date, freq='H').values, columns=['dt_hour'])\n",
    "    dt_hours['key'] = 0 #для cross join в merge \n",
    "\n",
    "    #все дни и часы для каждого региона (cross join)\n",
    "    data_hr = pd.merge(regions, dt_hours, how='outer')[['dt_hour','region']]\n",
    "\n",
    "    #добавим столбец с количеством поездок для каждой пары с помощью left join, заполнив NaN нулями\n",
    "    data_hr['trips'] = data_hr.merge(data_trips, how='left', on = ['dt_hour', 'region']).fillna(0).trips.astype(int)\n",
    "\n",
    "    #Подсчитываем среднее число поездок в час (по данным за май 2016, согласно 2-му заданию, оставляем 102 района)\n",
    "    if cut_regLess5 == True:\n",
    "        data_hr_mean = data_hr[(data_hr.dt_hour.dt.year==2016)&\n",
    "                               (data_hr.dt_hour.dt.month==5)].groupby('region', as_index=False)['trips'].mean()\n",
    "        regs_less5 = data_hr_mean[data_hr_mean.trips<5].region.values           #районы, где среднее число поездок меньше 5\n",
    "        data_hr.drop(data_hr[data_hr.isin({'region':regs_less5}).region ].index, inplace=True) #удаляем данные этих районов\n",
    "\n",
    "    #сохраняем файл\n",
    "    if save_data == True and save_fname <> '':\n",
    "        data_hr.to_csv(save_fname, index=False)\n",
    "    \n",
    "    #в принципе не обязательно, при выходе из функции сборка мусора удалит переменные\n",
    "#     del data_hr, dt_hours, data_trips, dfmon_list, data_hr_mean\n",
    "\n",
    "    if return_data == True:\n",
    "        return data_hr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
