{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"text-align:right\">Алексей Бебчик&nbsp;&nbsp;</div><b>\n",
    "### Курс 6. Желтое такси. Неделя 5. Прогнозирование с помощью регрессии  \n",
    "(задание, оцениваемое сокурсниками)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.coursera.org/learn/data-analysis-project/peer/YhEKS/proghnozirovaniie-s-pomoshch-iu-rieghriessii\">Задание</a>\n",
    "\n",
    "Класс моделей ARIMA недостаточно богат для наших данных: с их помощью, например, никак нельзя учесть взаимосвязи между рядами. Это можно сделать с помощью векторной авторегрессии VARIMA, но её питоновская реализация не позволяет использовать регрессионные признаки. Кроме того, авторегрессионный подход не позволяет учитывать, например, взаимодействия между сезонными компонентами. Вы могли заметить, что форма суточных сезонных профилей в будни и выходные немного разная; явно моделировать этот эффект с помощью ARIMA не получится. \n",
    "\n",
    "Нам нужна более сложная модель. Давайте займёмся сведением задачи массового прогнозирования рядов к регрессионной постановке!\n",
    "\n",
    "Вам понадобится много признаков. Некоторые из них у вас уже есть — это:  \n",
    "• идентификатор географической зоны;  \n",
    "• дата и время;  \n",
    "• количество поездок в периоды, предшествующие прогнозируемому;  \n",
    "• синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA.  \n",
    "\n",
    "Кроме того, не спешите выбрасывать построенный вами на прошлой неделе прогнозы — из них может получиться хороший признак для регрессии!  \n",
    "\n",
    "Вы можете попробовать разные регрессионный модели, но хорошие результаты, скорее всего, дадут такие, которые будут позволять признакам взаимодействовать друг с другом.  \n",
    "\n",
    "Поскольку прогноз нужен на 6 часов вперёд, проще всего будет построить 6 независимых регрессионных моделей — одна для прогнозирования  $\\hat{y}_{T+1∣T}$, другая для $\\hat{y}_{T+2∣T}$ и т.д.   \n",
    "\n",
    "1.Для каждой из шести задач прогнозирования $\\hat{y}_{T+i∣T},i=1,…,6$ сформируйте выборки. Откликом будет $y_{T+i}$ при всевозможных значениях $T$, а признаки можно использовать следующие:   \n",
    "• идентификатор географической зоны — категориальный;  \n",
    "• год, месяц, день месяца, день недели, час — эти признаки можно пробовать брать и категориальными, и непрерывными, можно даже и так, и так;  \n",
    "• синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA;  \n",
    "• сами значения прогнозов ARIMA $\\hat{y}_{T+i|T}^{ARIMA}$;  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T},y_{T-1}, \\dots, y_{T-K}$ (параметр $K$ можно подбирать; попробуйте начать, например, с 6);  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T-24}, y_{T-48}, \\dots, y_{T-24*K_d}$ (параметр $K_d$ можно подбирать; попробуйте начать, например, с 2);  \n",
    "• суммарное количество поездок из рассматриваемого района за предшествующие полдня, сутки, неделю, месяц.  \n",
    "\n",
    "Будьте внимательны при создании признаков — все факторы должны быть рассчитаны без использования информации из будущего: при прогнозировании $\\hat{y}_{T+i|T}, i=1,\\dots,6$ вы можете учитывать только значения $y$ до момента времени $T$ включительно.  \n",
    "\n",
    "2.Разбейте каждую из шести выборок на три части:   \n",
    "• обучающая, на которой будут настраиваться параметры моделей — всё до апреля 2016; \n",
    "• тестовая, на которой вы будете подбирать значения гиперпараметров — май 2016; \n",
    "• итоговая, которая не будет использоваться при настройке моделей вообще — июнь 2016. \n",
    "\n",
    "3.Выберите вашу любимую регрессионную модель и настройте её на каждом из шести наборов данных, подбирая гиперпараметры на мае 2016. Желательно, чтобы модель:  \n",
    "• допускала попарные взаимодействия между признаками;   \n",
    "• была устойчивой к избыточному количеству признаков (например, использовала регуляризаторы).   \n",
    "\n",
    "4.Выбранными моделями постройте для каждой географической зоны и каждого конца истории от 2016.04.30 23:00 до 2016.05.31 17:00 прогнозы на 6 часов вперёд; посчитайте в ноутбуке ошибку прогноза по следующему функционалу: \n",
    "$Q_{may} =\\frac1{R * 739* 6} \\sum\\limits_{r=1}^{R} \\sum_{T=2016.04.30 23:00}^{2016.05.31 17:00} \\sum_{i=1}^6 \\left| \\hat{y}^r_{T|T+i} - y^r_{T+i} \\right|$.  \n",
    "\n",
    "Убедитесь, что ошибка полученных прогнозов, рассчитанная согласно функционалу $Q$, определённому на прошлой неделе, уменьшилась по сравнению с той, которую вы получили методом индивидуального применения моделей ARIMA. Если этого не произошло, попробуйте улучшить ваши модели.  \n",
    "\n",
    "5.Итоговыми моделями постройте прогнозы для каждого конца истории от 2016.05.31 23:00 до 2016.06.30 17:00 и запишите все  результаты в один файл в формате $geoID, histEndDay, histEndHour, step, y$. Здесь $geoID$ — идентификатор зоны, $histEndDay$ — день конца истории в формате $id,y,$ где столбец $id$ состоит из склеенных через подчёркивание идентификатора географической зоны, даты конца истории, часа конца истории и номера отсчёта, на который делается предсказание (1-6); столбец $y$ — ваш прогноз.  \n",
    "\n",
    "6.Загрузите полученный файл на kaggle: https://inclass.kaggle.com/c/yellowtaxi. Добавьте в ноутбук ссылку на сабмишн.\n",
    "\n",
    "7.Загрузите ноутбук в форму.  \n",
    "\n",
    "__Review criteria__  \n",
    "В качестве ответа в этом задании вам нужно загрузить ноутбук; убедитесь, что ход анализа, который вы провели, описан достаточно подробно для того, чтобы ваши сокурсники поняли, что вы делали и почему.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import os                                     #для проверки существования файлов\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import datetime as dt\n",
    "import seaborn as sns \n",
    "import timeit as ti\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4_path = '..\\\\YTw4_data\\\\' #путь к папке с данными, неденя 4\n",
    "data5_path = 'YTw5_data\\\\'     #путь к папке с данными, неденя 5\n",
    "data_path  = '..\\\\YTData\\\\'    #путь к папке с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Имя файла с данными для формирования и загрузки\n",
    "reg_fname = data_path + 'regions.csv'\n",
    "w5_fname  = data5_path + 'YTw5' #K6w5\n",
    "w5_fname_data = w5_fname + '_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cload = True\n",
    "if cload==True: \n",
    "    data0 = pd.read_csv(w5_fname_data, parse_dates=['dt_hour'], dayfirst=True) #Загружаем подготовленные данные из файла\n",
    "else: #Готовим данные с сохраняем в файл\n",
    "    data0 = load_prepare_save_data(save_fname=w5_fname_data, reg_fname=reg_fname, month_count=6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЧАСТЬ 1. Признаки__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой из шести задач прогнозирования $ŷ T+i∣T,i=1,…,6$ $y^T+i∣T,i=1,…,6$ сформируйте выборки. Откликом будет  $yT+i$ при всевозможных значениях $T$ , а признаки можно использовать следующие:  \n",
    "• идентификатор географической зоны — категориальный;  \n",
    "• год, месяц, день месяца, день недели, час — эти признаки можно пробовать брать и категориальными, и непрерывными, можно даже и так, и так;  \n",
    "• синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA;  \n",
    "• сами значения прогнозов ARIMA $\\hat{y}_{T+i|T}^{ARIMA}$;  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T},y_{T-1}, \\dots, y_{T-K}$ (параметр $K$ можно подбирать; попробуйте начать, например, с 6);  \n",
    "• количество поездок из рассматриваемого района в моменты времени $y_{T-24}, y_{T-48}, \\dots, y_{T-24*K_d}$ (параметр $K_d$ можно подбирать; попробуйте начать, например, с 2);  \n",
    "• суммарное количество поездок из рассматриваемого района за предшествующие полдня, сутки, неделю, месяц.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Гиперпараметры __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tri_K        = 6  #Признак 3. Синусы, косинусы (6 оптимальное значение, выбранное Задании 3)\n",
    "reg_trips_K  = 12 #6 Признак 5. Количество поездок из района - без сезонности \n",
    "reg_trips_Ks = 4  #2 Признак 6. Количество поездок из района - недельная сезонность\n",
    "#Столбцы с датами, не допускаются в линейной регрессии\n",
    "cols_prev_dt  = ['prev_dt' +str(i) for i in range(1,reg_trips_K +1)]\n",
    "cols_prevS_dt = ['prevS_dt'+str(i) for i in range(1,reg_trips_Ks+1)]\n",
    "cols_dt       = ['dt', 'dt_hour', 'dt_y'] + cols_prev_dt + cols_prevS_dt #для data6\n",
    "cols_dt1      = ['dt', 'dt_hour'] + cols_prev_dt + cols_prevS_dt         #для data, без dt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cF1_beg = pd.datetime(2016,4,30, 23, 0) #начальная дата прогноза (Фолд 1)\n",
    "cF1_end = pd.datetime(2016,5,31, 17, 0) #конечная дата прогноза  (Фолд 1)\n",
    "cF2_beg = pd.datetime(2016,5,31, 23, 0) #начальная дата прогноза (Фолд 2)\n",
    "cF2_end = pd.datetime(2016,6,30, 17, 0) #конечная дата прогноза  (Фолд 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cF2_fin = cF2_end + pd.Timedelta(hours=6) #конечная дата прогноза (Фолд 2) с учетом прогноза на 6 часов вперед"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__При загрузке данных переходим на ЧАСТЬ 3. Обучение__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены, переходим к Часть 3. Обучение\n"
     ]
    }
   ],
   "source": [
    "cload = True\n",
    "if cload==True: \n",
    "    data6 = pd.read_csv(w5_fname+'_data6.csv', parse_dates=cols_dt, dayfirst=True) #Загружаем подготовленные данные из файла\n",
    "    print 'Данные загружены, переходим к Часть 3. Обучение'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 1. Идентификатор географической зоны — категориальный__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#Получаем преобразование в N столбцов, где каждый район - отдельный столбец, формируем DataFrame и добавляем к данным\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "reg_features = encoder.fit_transform(data0.region.values.reshape(-1,1)) \n",
    "reg_cat = pd.DataFrame(reg_features, columns=['reg_'+str(i) for i in encoder.active_features_], dtype=int)\n",
    "data = pd.concat([data0, reg_cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 2. год, месяц, день месяца, день недели, час (<font color='magenta'>категориальныме</font> и/или непрерывные)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# год, месяц, день месяца, день недели, час —  можно пробовать брать и категориальными, и непрерывными, можно даже и так, и так\n",
    "# неприрывные признаки:\n",
    "data['year']    = data.dt_hour.dt.year.astype(int)\n",
    "data['month']   = data.dt_hour.dt.month.astype(int)\n",
    "data['day']     = data.dt_hour.dt.day.astype(int) #можно и так: data.dt_hour.dt.floor('D') #выделим отдельно день\n",
    "data['hour']    = data.dt_hour.dt.hour.astype(int)\n",
    "data['weekday'] = data.dt_hour.dt.dayofweek      #день недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# категориальные признаки:\n",
    "for i in range(16 ,16+1): #год\n",
    "    data.loc[:, 'dtCatY'+str(i)] = 0\n",
    "    data.loc[data.year==i, 'dtCatY'+str(i)] = 1\n",
    "\n",
    "for i in range(1 ,12+1): #месяц\n",
    "    data.loc[:, 'dtCatM'+str(i)] = 0\n",
    "    data.loc[data.month==i, 'dtCatM'+str(i)] = 1\n",
    "\n",
    "for i in range(1 ,31+1): #день\n",
    "    data.loc[:, 'dtCatD'+str(i)] = 0\n",
    "    data.loc[data.day==i, 'dtCatD'+str(i)] = 1\n",
    "\n",
    "for i in range(1 ,24+1): #час\n",
    "    data.loc[:, 'dtCatH'+str(i)] = 0\n",
    "    data.loc[data.hour==i, 'dtCatH'+str(i)] = 1\n",
    "\n",
    "for i in range(1 ,7+1): #деньНедели\n",
    "    data.loc[:, 'dtCatWD'+str(i)] = 0\n",
    "    data.loc[data.weekday==i, 'dtCatWD'+str(i)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 3. Синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tri_cols = []\n",
    "for i in range(1,tri_K+1):\n",
    "    tri_cols.append('s'+str(i))\n",
    "    tri_cols.append('c'+str(i))\n",
    "\n",
    "rows_range = np.arange(1, len(data)+1)\n",
    "for i in range(1,tri_K+1):\n",
    "    data['s'+str(i)] = sin(rows_range*2*math.pi*i/168.0)\n",
    "    data['c'+str(i)] = cos(rows_range*2*math.pi*i/168.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 4. сами значения прогнозов ARIMA для $ŷT+i|T$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#прогноз модели SARIMAX за периоды обучения (апрель, май)\n",
    "sar_data = pd.read_csv(data4_path+'YTw4_tripdata_2016_pred.csv', parse_dates=['dt_hour'], dayfirst=True) \n",
    "sar_data.loc[:, 'hnum'] = np.array(sar_data.hour_num.apply(lambda x:x[-1:]), int) #числовой номер прогноза, понадобится\n",
    "# применение этих значений выполняется уже после размножения выборки на 6 частей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 5. Количество поездок из рассматриваемого района в моменты времени $y_{T},y_{T-1}, \\dots, y_{T-K}$ (параметр $K$ можно подбирать)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1,reg_trips_K+1):   \n",
    "    data['prev_dt'+str(i)] = data.dt_hour - pd.Timedelta(hours=i)\n",
    "    data.loc[:, 'prev_trips'+str(i)] = np.array(\n",
    "        pd.merge(data, data, how='left', left_on=['region', 'prev_dt'+str(i)], right_on=['region','dt_hour'])['trips_y'], int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font color=\"blue\">Внимание:</font>__ Здесь и далее значение в данных \"-2147483648\" означает __NA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 6. Количество поездок из рассматриваемого района в моменты времени $y_{T-24}, y_{T-48}, \\dots, y_{T-24*K_d}$ (параметр $K_d$ можно подбирать; попробуйте начать, например, с 2).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,reg_trips_Ks+1):   \n",
    "    data['prevS_dt'+str(i)] = data.dt_hour - pd.Timedelta(hours=24*i)\n",
    "    data.loc[:, 'prevS_trips'+str(i)] = np.array(\n",
    "        pd.merge(data, data, how='left', left_on=['region', 'prev_dt'+str(i)], right_on=['region','dt_hour'])['trips_y'], int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 7. суммарное количество поездок из рассматриваемого района за предшествующие полдня, сутки, неделю, месяц.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 703 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "load = True\n",
    "if load == True: \n",
    "     #Загружаем подготовленные данные из файла\n",
    "    prdsum_trips = pd.read_csv(data5_path + 'prdsum_trips.csv', parse_dates=['dt_hour'], dayfirst=True)\n",
    "else: \n",
    "    prd_trip_list = [] \n",
    "    dates = data.groupby('dt_hour').size().index.tolist() #список дат\n",
    "    print 'Всего:', len(dates)       #dates[10:11]:\n",
    "    for i, d in enumerate(dates[:]): #перебираем даты чтобы сгруппировать поездки за разные периоды до каждой даты\n",
    "        d_df    = data[data.dt_hour==d][['dt_hour', 'region']].copy() #регионы за текущую ДатуВремя (удобная основа для merge)\n",
    "        d_hday  = d - pd.Timedelta(hours = 12)\n",
    "        d_day   = d - pd.Timedelta(hours = 24)\n",
    "        d_week  = d - pd.Timedelta(weeks = 1 )\n",
    "        d_month = d - pd.Timedelta(weeks = 4 )\n",
    "\n",
    "        #print 'd {}, h/day {}, day {} week {} month {}'.format(d, d_hday, d_day, d_week, d_month)\n",
    "        df_hday  = data[(data.dt_hour >= d_hday ) & (data.dt_hour < d)].groupby('region', as_index=False)['trips'].sum()\n",
    "        df_day   = data[(data.dt_hour >= d_day  ) & (data.dt_hour < d)].groupby('region', as_index=False)['trips'].sum()\n",
    "        df_week  = data[(data.dt_hour >= d_week ) & (data.dt_hour < d)].groupby('region', as_index=False)['trips'].sum()\n",
    "        df_month = data[(data.dt_hour >= d_month) & (data.dt_hour < d)].groupby('region', as_index=False)['trips'].sum()\n",
    "\n",
    "        d_df['prdsum_hday']  = np.array(pd.merge(d_df, df_hday , how='left', on=['region'])['trips'], int) \n",
    "        d_df['prdsum_day' ]  = np.array(pd.merge(d_df, df_day  , how='left', on=['region'])['trips'], int) \n",
    "        d_df['prdsum_week']  = np.array(pd.merge(d_df, df_week , how='left', on=['region'])['trips'], int) \n",
    "        d_df['prdsum_month'] = np.array(pd.merge(d_df, df_month, how='left', on=['region'])['trips'], int) \n",
    "\n",
    "        prd_trip_list.append(d_df) #добавляем в список, один элемент - все регионы за одну дату\n",
    "\n",
    "        if i % 100 == 0: print i,  #индикация выполнения\n",
    "\n",
    "    prdsum_trips = pd.concat(prd_trip_list) #общий DataFrame со всеми регионами и датами\n",
    "\n",
    "    #prdsum_trips.to_csv('prdsum_trips.csv', index=False)  #сохраняем признаки (считается около 6 минут)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проставляем признаки в данных\n",
    "# data.drop(['prdsum_hday','prdsum_day','prdsum_week','prdsum_month'], axis=1, inplace=True)\n",
    "data.loc[:,'prdsum_hday']  = np.array(pd.merge(data, prdsum_trips, how='inner', on=['region', 'dt_hour'])['prdsum_hday'], int)\n",
    "data.loc[:,'prdsum_day']   = np.array(pd.merge(data, prdsum_trips, how='inner', on=['region', 'dt_hour'])['prdsum_day'], int)\n",
    "data.loc[:,'prdsum_week']  = np.array(pd.merge(data, prdsum_trips, how='inner', on=['region', 'dt_hour'])['prdsum_week'], int)\n",
    "data.loc[:,'prdsum_month'] = np.array(pd.merge(data, prdsum_trips, how='inner', on=['region', 'dt_hour'])['prdsum_month'], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Признак 8. Признаки погоды (пока не используем)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv(data_path + 'weather_data_nyc_centralpark_2016(1).csv', parse_dates=['date'], dayfirst=True)\n",
    "df_weather.rename(columns={u'date':u'dt'}, inplace=True)\n",
    "#Т.к. минимальное ненулевое значение 0.01, а T- это trace, \"след\", то поставим ему 0.001 (больше 0, меньше 0.01)\n",
    "df_weather['snow fall']     = df_weather['snow fall'].apply(lambda x: 0.001 if x == 'T' else float(x))\n",
    "df_weather['precipitation'] = df_weather['precipitation'].apply(lambda x: 0.001 if x == 'T' else float(x))\n",
    "#Добавим столбцы с погодой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['dt'] = data.dt_hour.dt.floor('D') #выделим отдельно день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['w_snow_fall'] = np.array(pd.merge(data, df_weather, how='inner', on=['dt'])['snow fall'])\n",
    "data['w_avg_temp']  = np.array(pd.merge(data, df_weather, how='inner', on=['dt'])['average temperature'])\n",
    "data['w_precip']    = np.array(pd.merge(data, df_weather, how='inner', on=['dt'])['precipitation']).astype(float)\n",
    "# Нормализация (от 0 до 1), не используем scale, т.к. отрицательные значения нам не нужны, их нет в sin и cos\n",
    "\n",
    "#Вообще это неправильная нормализация, нужно настроить Transformer и применятьь для прогноза уже его,\n",
    "#или также заново нормализовывать данные для прогноза\n",
    "data['w_snow_fall'] = np.array(data['w_snow_fall']) /np.array(data['w_snow_fall']).max().astype(float)\n",
    "data['w_precip'   ] = np.array(data['w_precip'   ]) /np.array(data['w_precip'   ]).max().astype(float)\n",
    "data['w_avg_temp' ] = np.array(data['w_avg_temp' ]) /np.array(data['w_avg_temp' ]).max().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "data.to_csv(w5_fname+'_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные с признаками загружены, переходим к делению на 6 часов\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cload = False \n",
    "cload = True \n",
    "if cload==True: \n",
    "    data = pd.read_csv(w5_fname+'_data1.csv', parse_dates=cols_dt1, dayfirst=True) #Загружаем подготовленные данные из файла\n",
    "    print 'Данные с признаками загружены, переходим к делению на 6 часов'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЧАСТЬ 2. Выборки__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Выборки 6шт. __ Для каждой из шести задач прогнозирования сформируйте выборки, откликом будет  $yT+i$ для всех возможных значениях $T$:\n",
    "\n",
    "__Разбейте каждую из шести выборок на три части:__  \n",
    "• обучающая, на которой будут настраиваться параметры моделей — всё до апреля 2016;   \n",
    "• тестовая, на которой вы будете подбирать значения гиперпараметров — май 2016;   \n",
    "• итоговая, которая не будет использоваться при настройке моделей вообще — июнь 2016.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Сначалот разобъем выборку на 3 части, т.к. формирование 6 выборок далее никак не должно повлиять на результат\n",
    "data['part'] = 1                                            #Часть 1 для обучения\n",
    "data.loc[data.dt_hour >= dt.datetime(2016,5,1), 'part'] = 2 #Часть 2 для настройки гиперпапаметров\n",
    "data.loc[data.dt_hour >= dt.datetime(2016,6,1), 'part'] = 3 #Часть 3 для предсказания\n",
    "\n",
    "data.loc[:, 'cF'] = 0 #Часть 3 для предсказания\n",
    "data.loc[(data.dt_hour >= cF1_beg) & (data.dt_hour <= cF1_end), 'cF'] = 1 #Прогноз Май\n",
    "data.loc[(data.dt_hour >= cF2_beg) & (data.dt_hour <= cF2_end), 'cF'] = 2 #Прогноз Июнь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cF1 2016-04-30 23:00:00 2016-05-31 17:00:00\n",
      "cF2 2016-05-31 23:00:00 2016-06-30 17:00:00\n"
     ]
    }
   ],
   "source": [
    "print 'cF1', cF1_beg, cF1_end\n",
    "print 'cF2', cF2_beg, cF2_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Теперь формируем 6 выборок, в кажой строке пронозируемая величина y - значение текущей ДатаВремя + 1-6 часов к текущему времени\n",
    "df_list = []\n",
    "for i in range(1,7): \n",
    "    df = data.copy()\n",
    "    df.loc[:, 'num6'] = i\n",
    "    df.loc[:, 'dt_y'] = df.dt_hour + pd.Timedelta(hours=i) #     data['T' +str(i)] = data.hour + i\n",
    "    df.loc[:, 'y'] = np.array(pd.merge(df, df, how='left', left_on=['region', 'dt_y'], \n",
    "                                          right_on=['region','dt_hour'])['trips_y'], int)\n",
    "    df.loc[:, 'dt_start'] = df.loc[:, 'dt_hour']\n",
    "\n",
    "    #реальные значения прогноза SARIMAX, в т.ч. отрицательные\n",
    "    df.loc[:, 'y_sar'] = np.array(pd.merge(df, sar_data[sar_data.hnum==i], how='left', \n",
    "                                               left_on=['region', 'dt_y'],\n",
    "                                              right_on=['region', 'dt_hour'])['pred_trips'], int)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "data6 = pd.concat(df_list) #сформируем единый датафрейм, так удобнее\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data6['future'] = 0 #признак использования будущих значений, для фильтрации во время обучения\n",
    "data6.loc[(data6.part == 1) & (data6.dt_y >= dt.datetime(2016,5,1)), 'future'] = 1  \n",
    "data6.loc[(data6.part == 2) & (data6.dt_y >= dt.datetime(2016,6,1)), 'future'] = 1\n",
    "data6.loc[(data6.part == 3) & (data6.dt_y >= dt.datetime(2016,7,1)), 'future'] = 1  #в общем-то, таких нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data6.to_csv(w5_fname+'_data6.csv', index=False) #сохраним в файл, чтобы каждый раз не считать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЧАСТЬ 3. Обучение__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Выберите вашу любимую регрессионную модель и настройте её на каждом из шести наборов данных, подбирая гиперпараметры на мае 2016. Желательно, чтобы модель:  \n",
    "• допускала попарные взаимодействия между признаками;   \n",
    "• была устойчивой к избыточному количеству признаков (например, использовала регуляризаторы).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols1_reg = ['reg_'+str(i) for i in data.region.unique()] #1.Регионы\n",
    "cols2_dtNum = ['year', 'month', 'day', 'hour', 'weekday']  #2.Время (неприрывные признаки)\n",
    "#2.Время (категориальные признаки)\n",
    "cols2_dtCat       = ['dtCatY' +str(i) for i in range(16,16+1)] #год\n",
    "cols2_dtCat      += ['dtCatM' +str(i) for i in range(1 ,12+1)] #месяц\n",
    "cols2_dtCat      += ['dtCatD' +str(i) for i in range(1 ,31+1)] #день\n",
    "cols2_dtCat      += ['dtCatH' +str(i) for i in range(1 ,24+1)] #час\n",
    "cols2_dtCat      += ['dtCatWD'+str(i) for i in range(1 ,7 +1)] #деньНедели\n",
    "cols3_prev_trips  = ['prev_trips' +str(i) for i in range(1,reg_trips_K+1)]  #3. Прямые предыдущие поездки\n",
    "cols4_prevS_trips = ['prevS_trips'+str(i) for i in range(1,reg_trips_Ks+1)] #4. Сезонно предыдущие поездки\n",
    "cols5_tri         = ['s'+str(i) for i in range(1,tri_K+1)]   #5. Тригонометрические признаки (синусы)\n",
    "cols5_tri        += ['c'+str(i) for i in range(1,tri_K+1)]   #5. Тригонометрические признаки (косинусы)\n",
    "cols6_sar         = ['y_sar']                #6. Прогнозы SARIMAX (май, июнь)\n",
    "cols7_prdsum      = ['prdsum_hday', 'prdsum_day', 'prdsum_week', 'prdsum_month'] #7. Агрегаты прошлых поездок\n",
    "cols8_weather     = ['w_precip', 'w_snow_fall', 'w_avg_temp'] #8. Признаки погоды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Гиперпараметры:__  \n",
    "• год, месяц, день месяца, день недели, час: 1) категориальные, 2) непрерывные, 3) и так, и так;    \n",
    "• количество поездок из района в моменты времени $y_{T},y_{T-1}, \\dots, y_{T-K}$: $K$ [6, ...];  \n",
    "• количество поездок из района в моменты времени $y_{T-24}, y_{T-48}, \\dots, y_{T-24*K_d}$: $K_d$ [2,...];    \n",
    "• регуляризатор alpha = [1, 200, 400, 600].  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pK     = range(6,12+1)    #количество поездок из района K \n",
    "pKd    = range(2,4+1)     #количество поездок из района Kd \n",
    "pAlpha = [100, 200, 400]  #регуляризатор alpha\n",
    "pDtCat = [0,1]            #1) категориальные, 2) непрерывные, 3) и так, и так;\n",
    "pDtNum = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cload = False\n",
    "if cload == False:\n",
    "    #Первоначальное наполнение по диапазонам\n",
    "    hprms_dic  = {'K':pK,'Kd':pKd,'Alpha':pAlpha, 'dtCat':pDtCat, 'dtNum': pDtNum}\n",
    "    hprms_data = list(itertools.product(hprms_dic['K'], hprms_dic['Kd'], hprms_dic['Alpha'], \n",
    "                                        hprms_dic['dtCat'], hprms_dic['dtNum'],\n",
    "                                        [0], [0], [0.0], [0], [0.0]))    \n",
    "    hprms = pd.DataFrame(columns = ['K','Kd','Alpha','dtCat', 'dtNum', 'done', 'time', 'qual', 'exp_time', 'exp_qual'], \n",
    "                         data = hprms_data)\n",
    "    hprms.to_csv('w5_prms.csv', index=False)\n",
    "else:\n",
    "    hprms = pd.read_csv(data5_path + 'w5_prms.csv', sep=',') #из отдельных файлов\n",
    "    hprms.drop_duplicates(inplace=True) #если загрузили два раза случайно, могут быть возникнуть дубли "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Удалим данные без категориальных и неприрывных признаков ДатаВремя одновременно\n",
    "hprms.drop(hprms[(hprms.dtCat==0) & (hprms.dtNum==0)].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Перебираем значения гиперпараметров, обучаем все 6 выборок, рассчитываем качество обучения.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Только для подбора параметров\n",
    "fit_hprms(m_data=data6, m_prms=hprms, m_count=10, fname=data5_path + 'w5_hprms.csv') #запись после каждой комбинации m_cN=cN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "1    189\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print hprms.groupby(['done'], as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Выбор оптимальных параметров__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Kd</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>dtCat</th>\n",
       "      <th>dtNum</th>\n",
       "      <th>done</th>\n",
       "      <th>time</th>\n",
       "      <th>qual</th>\n",
       "      <th>exp_time</th>\n",
       "      <th>exp_qual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>52.121946</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>52.332008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>52.342901</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>52.575721</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>52.821325</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   K  Kd  Alpha  dtCat  dtNum  done  time       qual  exp_time  exp_qual\n",
       "6  6   2    400      1      0     1  0.10  52.121946         0       0.0\n",
       "5  6   2    400      0      1     1  0.07  52.332008         0       0.0\n",
       "1  6   2    200      0      1     1  0.08  52.342901         0       0.0\n",
       "2  6   2    200      1      0     1  0.11  52.575721         0       0.0\n",
       "3  6   2    200      1      1     1  0.10  52.821325         0       0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprms[hprms.done==1].sort_values(by=['qual', 'K', 'Kd', 'dtCat', 'dtNum'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#P.S. после был еще добавлен регуляризатор 600, он оказался лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вывод:__  \n",
    "K     = 12 зависимость от предыдущих значений: выбрано максимальное значение, видимо, чем больше, тем лучше;  \n",
    "K     = 2  зависимость от предыдущих сезонных значений: все значения 2,3,4 не влияют на результат;  \n",
    "Alpha = 600 регуляризация: максимальное значение, видимо, чем больше, тем лучше;  \n",
    "dtNum = 1 - ДатаВремя - непрерывные признаки: дают значительный вклад в качество.  \n",
    "dtCat = 0 - ДатаВремя - категориальные признаки: не дают явного вклада в качество модели.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЧАСТЬ 4.__ Выбранными моделями постройте для каждой географической зоны и каждого конца истории от 2016.04.30 23:00 до 2016.05.31 17:00 прогнозы на 6 часов вперёд; посчитайте в ноутбуке ошибку прогноза по следующему функционалу: \n",
    "$Q_{may} =\\frac1{R * 739* 6} \\sum\\limits_{r=1}^{R} \\sum_{T=2016.04.30 23:00}^{2016.05.31 17:00} \\sum_{i=1}^6 \\left| \\hat{y}^r_{T|T+i} - y^r_{T+i} \\right|$.  \n",
    "\n",
    "Убедитесь, что ошибка полученных прогнозов, рассчитанная согласно функционалу $Q$, определённому на прошлой неделе, уменьшилась по сравнению с той, которую вы получили методом индивидуального применения моделей ARIMA. Если этого не произошло, попробуйте улучшить ваши модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfin5may = data6[(data6.dt_hour>=cF1_beg)&(data6.dt_hour<=cF1_end)] #данные для прогноза за май"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.56150026126884\n"
     ]
    }
   ],
   "source": [
    "#Еще раз обучаем и получаем модель с нужными гиперпараметрами\n",
    "hprm_K     = 12\n",
    "hprm_Kd    = 2\n",
    "hprm_Alpha = 600 \n",
    "hprm_dtCat = 0\n",
    "hprm_dtNum = 1\n",
    "\n",
    "q0, lr_list0, ds0 = fit_model_fin5(m_dt=dfin5may,\n",
    "                         m_K=hprm_K, m_Kd=hprm_Kd, m_Alpha=hprm_Alpha, m_dtCat=hprm_dtCat, m_dtNum=hprm_dtNum)\n",
    "print q0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика __Q__  на первом Фолде (май) SARIMAX: __27.78__, а здесь получилось __48.56__, хуже...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__РАЗДЕЛ 5__.   \n",
    "Итоговыми моделями постройте прогнозы для каждого конца истории от 2016.05.31 23:00 до 2016.06.30 17:00 и запишите все  результаты в один файл в формате $geoID, histEndDay, histEndHour, step, y$. Здесь $geoID$ — идентификатор зоны, $histEndDay$ — день конца истории в формате $id,y,$ где столбец $id$ состоит из склеенных через подчёркивание идентификатора географической зоны, даты конца истории, часа конца истории и номера отсчёта, на который делается предсказание (1-6); столбец $y$ — ваш прогноз.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Даты прогноза 2016-05-31 23:00:00 2016-06-30 17:00:00\n"
     ]
    }
   ],
   "source": [
    "print 'Даты прогноза', cF2_beg, cF2_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfin5 = data6[(data6.dt_hour>=cF2_beg)&(data6.dt_hour<=cF2_end)] #данные для прогноза за июнь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437580"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfin5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.440455840539904\n"
     ]
    }
   ],
   "source": [
    "hprm_K     = 12\n",
    "hprm_Kd    = 2\n",
    "hprm_Alpha = 600 \n",
    "hprm_dtCat = 0\n",
    "hprm_dtNum = 1\n",
    "\n",
    "q0, lr_list0, ds_list0 = fit_model_fin5(m_dt=dfin5, m_K=hprm_K, \n",
    "                                        m_Kd=hprm_Kd, m_Alpha=hprm_Alpha, m_dtCat=hprm_dtCat, m_dtNum=hprm_dtNum)\n",
    "print q0\n",
    "\n",
    "pred = pd.concat(ds_list0) #собираем в один датафрейм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика __Q__  на втором Фолде (июнь) SARIMAX: __36.83__, а здесь получилось __37.44__, несколько хуже...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437580, 150)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# формируем идентификатор: Регион_ДатаЧас_ЧасПрозноза\n",
    "pred['save_id'] = pred.region.astype(int).astype(str) + '_'+pred.dt_hour.dt.strftime('%Y-%m-%d_%H')\n",
    "# если в часе даты конца истории встречается 0, например, 03, то надо оставить 3 (если 00, то будет 0)\n",
    "pred['save_id'] = pred.save_id.apply(lambda x:x[:-2]+x[-1:] if x[-2:-1]=='0' else x)\n",
    "# добавляем номер прогнозного часа от 1 до 6\n",
    "pred['save_id'] = pred['save_id'] + '_' + pred.num6.astype(str) #.apply(lambda x:x[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сформируем файл с ответом\n",
    "kaggle_answer5 = pred[['save_id','y_pred']]\n",
    "kaggle_answer5.columns = ['id','y']\n",
    "kaggle_answer5.to_csv(data5_path + 'Alexey_Bebchik_k6w5_answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ВЫВОД__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе была построена система автматизированного расчета подбора параметров модели, позволяющая остнавливать и продолжать расчтеы в произвольные моменты времени (fit_hprms). \n",
    "\n",
    "Проведен анализ и выделены оптимальные варианты. К сожалению, значения метрик пока не удалось сделать лучше предыдущих значений, полученных при помощи SARIMAX. Возможно, это удастся сделать в работе следующей недели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл ответа загружен на __Kaggle__:  __AlexModern__, 91 место (score 36.82). Есть, к чему стремиться. \n",
    "Приложен скриншот: __Alexey_Bebchik_Kaggle_Leaderbord_Week5(AlexModern)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Блок определения пользовательских функций__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция предварительного обучения для подбора параметров SARIMAX\n",
    "def fit_hprms(m_data, m_prms, m_count=None, #m_cN,\n",
    "                     fname='', save_prms=True, m_new_only=True, verbose=True, fit_exp=False): \n",
    "    step      = 0 \n",
    "    wall_len  = 0\n",
    "    m_prms_search = m_prms[(m_prms.done==0)|(m_new_only!=True)] #new_only - только параметры, на которых пока не обучались\n",
    " \n",
    "\n",
    "    #перебираем параметры обучения (если модель еще не обучена и exp_time=0, оставим сортировку по параметрам)\n",
    "    for i, r in m_prms_search.sort_values(by=['exp_time','K','Kd','Alpha','dtCat','dtNum'], \n",
    "                                          ascending=True)[:m_count].iterrows():\n",
    "        step += 1\n",
    "        m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum = int(r.K), int(r.Kd), float(r.Alpha), int(r.dtCat), int(r.dtNum) #параметры обучения\n",
    "\n",
    "        #тут определяем mcols на базе m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum\n",
    "        m_cols=['K','Kd','Alpha','dtCat', 'dtNum']\n",
    "        \n",
    "        t = ti.default_timer()                         #время начала обучения\n",
    "\n",
    "        m_q0, m_lr0, ds0 = fit_model(2, m_data, m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum) #обучение на параметрах fit_sarimax\n",
    "        \n",
    "        step_len = round((ti.default_timer()-t)/60.,2) #длительность обучения\n",
    "        wall_len  += step_len                          #суммарная длительность обучения  \n",
    "\n",
    "        #индекс текущей строки в общем наборе параметров\n",
    "        m_prms_index = m_prms[(m_prms.K==m_K) & (m_prms.Kd==m_Kd) & (m_prms.Alpha==m_Alpha) & \n",
    "                              (m_prms.dtCat==m_dtCat) & (m_prms.dtNum==m_dtNum)].index\n",
    "\n",
    "        m_prms.loc[m_prms_index, 'done'] = 1         #признак обработанной строки \n",
    "        m_prms.loc[m_prms_index, 'qual'] = m_q0      #сохраняем Q  \n",
    "        m_prms.loc[m_prms_index, 'time'] = step_len  #сохраняем длительность выполнения \n",
    "        \n",
    "        if verbose == True: \n",
    "            print '{}/{} time {} ({}) K{} Kd{} Alpha{} dtCat{} dtNum{} Q{}'.format( \n",
    "                step,\n",
    "                m_prms_search.shape[0], \n",
    "                round(wall_len/60.,2), \n",
    "                step_len, \n",
    "                m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum, m_q0) \n",
    "#             print 'index', m_prms_index\n",
    "\n",
    "    #по завершении расчета записываем параметры\n",
    "    if save_prms == True and fname <> '':\n",
    "        m_prms.to_csv(fname, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция обучения модели для предварительного обучения для подбора параметров SARIMAX\n",
    "def fit_model(m_part, m_dt, m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum):    \n",
    "\n",
    "    mQ_delta = 0.0\n",
    "    mQ_shape = 0.0\n",
    "    mlr_list = []\n",
    "    ds_list  = []\n",
    "    aux_cols = ['region', 'dt_hour', 'dt_y', 'num6']\n",
    "    \n",
    "    for i in range(1,7)[:]: \n",
    "        \n",
    "        lr_cols = aux_cols + cols1_reg + cols5_tri + cols6_sar + cols7_prdsum + cols8_weather  #cols6_sarF2 ?\n",
    "        lr_cols += cols3_prev_trips[:m_K]\n",
    "        lr_cols += cols4_prevS_trips[:m_Kd]\n",
    "        if m_dtNum == 1: \n",
    "            lr_cols += cols2_dtNum \n",
    "        if m_dtCat == 1: \n",
    "            lr_cols += cols2_dtCat\n",
    "\n",
    "        #часть для подбора гиперпараметров и обучения модели\n",
    "        ds = m_dt[(m_dt.num6 == i) & (m_dt.part==m_part) & (m_dt.future == 0)][lr_cols+['y']] #.drop(dt_clos, axis=1) \n",
    "        \n",
    "        lr = linear_model.Ridge(alpha=m_Alpha) \n",
    "        \n",
    "        lr.fit(ds.drop(aux_cols+['y'], axis=1), ds.y)\n",
    "        \n",
    "        ds.loc[:, 'y_pred'] = lr.predict(ds.drop(aux_cols+['y'], axis=1))\n",
    "        \n",
    "        ds.loc[:, 'delta_y'] = ds.loc[:, 'y_pred'] - ds.loc[:, 'y']\n",
    "\n",
    "        ds.loc[:, 'delta_y_abs'] = ds.loc[:,'delta_y'].abs()\n",
    "\n",
    "        m_Q = sum(ds.loc[:, 'delta_y_abs'])/float(ds.shape[0]) # cXpred.groupby('region').delta_trips_abs.sum()/(739.*6.)\n",
    "        \n",
    "        mQ_delta += sum(ds.loc[:, 'delta_y_abs'])\n",
    "        mQ_shape += float(ds.shape[0])\n",
    "\n",
    "        mlr_list.append(lr)\n",
    "        ds_list.append(ds)\n",
    "        \n",
    "#         MAE = metrics.mean_absolute_error(np.array(ds.y), np.array(ds.y_pred))\n",
    "\n",
    "    mQall = mQ_delta/mQ_shape\n",
    "\n",
    "    return (mQall, mlr_list, ds_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция для обучения модели и предсказания значений (при предсказании не вырезаются данные \"будущего\" future=1)\n",
    "def fit_model_fin5(m_dt, m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum):    \n",
    "\n",
    "    mQ_delta = 0.0\n",
    "    mQ_shape = 0.0\n",
    "    mlr_list = []\n",
    "    ds_list  = []\n",
    "    aux_cols = ['region', 'dt_hour', 'dt_y', 'num6', 'future']\n",
    "    \n",
    "    for i in range(1,7)[:]: \n",
    "        \n",
    "        lr_cols = aux_cols + cols1_reg + cols5_tri + cols6_sar + cols7_prdsum + cols8_weather  #cols6_sarF2 ?\n",
    "        lr_cols += cols3_prev_trips[:m_K]\n",
    "        lr_cols += cols4_prevS_trips[:m_Kd]\n",
    "        if m_dtNum == 1: \n",
    "            lr_cols += cols2_dtNum \n",
    "        if m_dtCat == 1: \n",
    "            lr_cols += cols2_dtCat\n",
    "\n",
    "        ds = m_dt[(m_dt.num6 == i)][lr_cols+['y']] #.drop(dt_clos, axis=1) \n",
    "\n",
    "        lr = linear_model.Ridge(alpha=m_Alpha) \n",
    "        \n",
    "        lr.fit(ds[(ds.future == 0)].drop(aux_cols+['y'], axis=1), ds[(ds.future == 0)].y)\n",
    "       \n",
    "        ds.loc[:, 'y_pred'] = lr.predict(ds.drop(aux_cols+['y'], axis=1))\n",
    "        \n",
    "        ds.loc[:, 'delta_y'] = ds.loc[:, 'y_pred'] - ds.loc[:, 'y']\n",
    "\n",
    "        ds.loc[:, 'delta_y_abs'] = ds.loc[:,'delta_y'].abs()\n",
    "\n",
    "        m_Q = sum(ds.loc[:, 'delta_y_abs'])/float(ds.shape[0]) # cXpred.groupby('region').delta_trips_abs.sum()/(739.*6.)\n",
    "        \n",
    "        mQ_delta += sum(ds.loc[:, 'delta_y_abs'])\n",
    "        mQ_shape += float(ds.shape[0])\n",
    "\n",
    "        mlr_list.append(lr)\n",
    "        ds_list.append(ds)\n",
    "\n",
    "\n",
    "    mQall = mQ_delta/mQ_shape\n",
    "\n",
    "    return (mQall, mlr_list, ds_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция предварительного обучения для подбора параметров SARIMAX\n",
    "def fit_hprms(m_data, m_prms, m_count=None, #m_cN,\n",
    "                     fname='', save_prms=True, m_new_only=True, verbose=True, fit_exp=False): \n",
    "    step      = 0 \n",
    "    wall_len  = 0\n",
    "    m_prms_search = m_prms[(m_prms.done==0)|(m_new_only!=True)] #new_only - только параметры, на которых пока не обучались\n",
    " \n",
    "\n",
    "    #перебираем параметры обучения (если модель еще не обучена и exp_time=0, оставим сортировку по параметрам)\n",
    "    for i, r in m_prms_search.sort_values(by=['exp_time','K','Kd','Alpha','dtCat','dtNum'], \n",
    "                                          ascending=True)[:m_count].iterrows():\n",
    "        step += 1\n",
    "        m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum = int(r.K), int(r.Kd), float(r.Alpha), int(r.dtCat), int(r.dtNum)#параметры обучения\n",
    "\n",
    "        #тут определяем mcols на базе m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum\n",
    "        m_cols=['K','Kd','Alpha','dtCat', 'dtNum']\n",
    "        \n",
    "        t = ti.default_timer()                         #время начала обучения\n",
    "\n",
    "        m_q0, m_lr0, ds0 = fit_model(2, m_data, m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum) #обучение на параметрах fit_sarimax\n",
    "        \n",
    "        step_len = round((ti.default_timer()-t)/60.,2) #длительность обучения\n",
    "        wall_len  += step_len                          #суммарная длительность обучения  \n",
    "\n",
    "        #индекс текущей строки в общем наборе параметров\n",
    "        m_prms_index = m_prms[(m_prms.K==m_K) & (m_prms.Kd==m_Kd) & (m_prms.Alpha==m_Alpha) & \n",
    "                              (m_prms.dtCat==m_dtCat) & (m_prms.dtNum==m_dtNum)].index\n",
    "\n",
    "        m_prms.loc[m_prms_index, 'done'] = 1         #признак обработанной строки \n",
    "        m_prms.loc[m_prms_index, 'qual'] = m_q0      #сохраняем Q  \n",
    "        m_prms.loc[m_prms_index, 'time'] = step_len  #сохраняем длительность выполнения \n",
    "        \n",
    "        if verbose == True: \n",
    "            print '{}/{} time {} ({}) K{} Kd{} Alpha{} dtCat{} dtNum{} Q{}'.format( \n",
    "                step,\n",
    "                m_prms_search.shape[0], \n",
    "                round(wall_len/60.,2), \n",
    "                step_len, \n",
    "                m_K, m_Kd, m_Alpha, m_dtCat, m_dtNum, m_q0) \n",
    "#             print 'index', m_prms_index\n",
    "\n",
    "    #по завершении расчета записываем параметры\n",
    "    if save_prms == True and fname <> '':\n",
    "        m_prms.to_csv(fname, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_time(label='now'):\n",
    "    print label, dt.datetime.now().strftime('%H:%M')\n",
    "    \n",
    "#Готовим данные и записываем их в файл (примерно 6 минут да 2016 год)\n",
    "def load_prepare_save_data(save_fname, month_count, reg_fname= data_path + 'regions.csv', \n",
    "                           cut_regLess5 = True, save_data=True, return_data=True):\n",
    "    \n",
    "    #Координаты квадрата Нью_Йорка\n",
    "    west_ny, east_ny, south_ny, north_ny  = -74.25559,-73.700018, 40.49612, 40.91553\n",
    "    \n",
    "    #Загружаем районы и получаем границы корзин\n",
    "    regs = pd.read_csv(reg_fname, sep=';') \n",
    "    binx  = sorted(set(regs.west.value_counts().index)  | set(regs.east.value_counts().index ))\n",
    "    biny  = sorted(set(regs.south.value_counts().index) | set(regs.north.value_counts().index))\n",
    "    \n",
    "    \n",
    "    # Составим список файлов для обработки\n",
    "    files = [data_path + 'yellow_tripdata_2016-' + ('0'+str(x+1))[-2:] + '.csv' for x in range(month_count)]# [:1] для отладкаи 5\n",
    "\n",
    "    #Загрузим данные, ограничив их по району, количеству пассажиров и продолжительности поездки\n",
    "    data_cols = ['tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count',\n",
    "                 'trip_distance','pickup_longitude','pickup_latitude'] \n",
    "\n",
    "    dfmon_list = []\n",
    "    for f in files:\n",
    "        print_time('read '+ f)\n",
    "        dfmon = pd.read_csv(f, sep=',', parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'], usecols=data_cols) \n",
    "        #print_time('get del_index')\n",
    "        del_index = dfmon[(dfmon.tpep_pickup_datetime == dfmon.tpep_dropoff_datetime) #Нулевая длительность\n",
    "                    | (dfmon['passenger_count' ] == 0)          # поездки с нулевым количеством пассажиров\n",
    "                    | (dfmon['trip_distance'   ] == 0)          # поездки с нулевым расстоянием поездки по счётчику\n",
    "                    | (dfmon['pickup_longitude'] <  west_ny)    # поездки, не попадающие в прямоугольник Нью-Йорка\n",
    "                    | (dfmon['pickup_longitude'] >= east_ny)    \n",
    "                    | (dfmon['pickup_latitude' ] <  south_ny)   \n",
    "                    | (dfmon['pickup_latitude' ] >= north_ny)  \n",
    "                         ].index\n",
    "        #print_time('drop del_index')\n",
    "        dfmon.drop(del_index, inplace=True)\n",
    "        dfmon['dt_hour'] = dfmon.tpep_pickup_datetime.dt.floor('H') #день и час (.dt.hour - только час)\n",
    "        #print_time('calc reg bins')\n",
    "        #Разобъем данные на квадраты\n",
    "        r_bins = binned_statistic_2d(\n",
    "            x = dfmon.pickup_longitude, \n",
    "            y = dfmon.pickup_latitude, \n",
    "            values = None, \n",
    "            statistic = 'count', \n",
    "            bins      = [binx, biny],\n",
    "            range     = [[-74.25559, -73.70001], [40.49612, 40.91553]], expand_binnumbers = True)\n",
    "        # Вычислим номер региона\n",
    "        bin_regs = np.array([(x-1) * 50 + y for x, y in zip(*r_bins.binnumber)])\n",
    "        #print_time('set regs')\n",
    "        dfmon['region']  = bin_regs\n",
    "        #print_time('aggregate')\n",
    "        dfmon_agr = dfmon.groupby(['dt_hour', 'region'], as_index=False).pickup_longitude.count()\n",
    "        dfmon_agr.columns = ['dt_hour', 'region', 'trips']\n",
    "        dfmon_list.append(dfmon_agr)\n",
    "\n",
    "    data_trips = pd.concat(dfmon_list) #только совершенные поездки в регионах\n",
    "\n",
    "    #---- надо добавить нули для регионов и часов, в которых не было поездок\n",
    "\n",
    "    #все регионы\n",
    "    regions  = pd.DataFrame(regs.region, columns=['region'])\n",
    "    regions['key'] = 0 #для cross join в merge \n",
    "\n",
    "    #все дни и часы за весь период в данных\n",
    "    start_date  = data_trips.dt_hour.min() #дата начала данных\n",
    "    end_date    = data_trips.dt_hour.max() #дата окончания данных\n",
    "    dt_hours = pd.DataFrame(pd.date_range(start_date, end_date, freq='H').values, columns=['dt_hour'])\n",
    "    dt_hours['key'] = 0 #для cross join в merge \n",
    "\n",
    "    #все дни и часы для каждого региона (cross join)\n",
    "    data_hr = pd.merge(regions, dt_hours, how='outer')[['dt_hour','region']]\n",
    "\n",
    "    #добавим столбец с количеством поездок для каждой пары с помощью left join, заполнив NaN нулями\n",
    "    data_hr['trips'] = data_hr.merge(data_trips, how='left', on = ['dt_hour', 'region']).fillna(0).trips.astype(int)\n",
    "\n",
    "    #Подсчитываем среднее число поездок в час (по данным за май 2016, согласно 2-му заданию, оставляем 102 района)\n",
    "    if cut_regLess5 == True:\n",
    "        data_hr_mean = data_hr[(data_hr.dt_hour.dt.year==2016)&\n",
    "                               (data_hr.dt_hour.dt.month==5)].groupby('region', as_index=False)['trips'].mean()\n",
    "        regs_less5 = data_hr_mean[data_hr_mean.trips<5].region.values           #районы, где среднее число поездок меньше 5\n",
    "        data_hr.drop(data_hr[data_hr.isin({'region':regs_less5}).region ].index, inplace=True) #удаляем данные этих районов\n",
    "\n",
    "    #сохраняем файл\n",
    "    if save_data == True and save_fname <> '':\n",
    "        data_hr.to_csv(save_fname, index=False)\n",
    "    \n",
    "    if return_data == True:\n",
    "        return data_hr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
